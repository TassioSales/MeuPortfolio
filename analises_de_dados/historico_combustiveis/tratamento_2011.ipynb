{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61877e1",
   "metadata": {},
   "source": [
    "# üßπ Tratamento de Dados de Combust√≠veis - 2011\n",
    "\n",
    "Este notebook tem como objetivo realizar o tratamento inicial dos dados de pre√ßos de combust√≠veis da ANP para o ano de 2011. Aqui, vamos garantir que os dados estejam limpos e estruturados para an√°lises futuras.\n",
    "\n",
    "## Objetivos:\n",
    "- Carregar os dados dos dois semestres de 2011\n",
    "- Verificar a consist√™ncia das colunas\n",
    "- Identificar e tratar valores ausentes\n",
    "- Preparar os dados para an√°lises posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0266cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "caminho_1s = 'dados_anp_ca/ca-2011-01.csv'\n",
    "caminho_2s = 'dados_anp_ca/ca-2011-02.csv'\n",
    "\n",
    "def exibir_colunas(df, nome):\n",
    "    print(f\"\\nüìã Colunas do {nome}:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        tipo = str(df[col].dtype)\n",
    "        print(f\"{i:2d}. {col:<40} ({tipo})\")\n",
    "    print(f\"\\nTotal: {len(df.columns)} colunas\")\n",
    "\n",
    "# Lendo os arquivos CSV\n",
    "try:\n",
    "    # Lendo o primeiro semestre\n",
    "    print(\"üìÇ Carregando dados do primeiro semestre...\")\n",
    "    df_1s = pd.read_csv(caminho_1s, encoding='utf-8', sep=';', decimal=',')\n",
    "    print(f\"‚úÖ Registros carregados: {len(df_1s):,}\")\n",
    "    exibir_colunas(df_1s, \"Primeiro Semestre\")\n",
    "    \n",
    "    # Lendo o segundo semestre\n",
    "    print(\"\\nüìÇ Carregando dados do segundo semestre...\")\n",
    "    df_2s = pd.read_csv(caminho_2s, encoding='utf-8', sep=';', decimal=',')\n",
    "    print(f\"‚úÖ Registros carregados: {len(df_2s):,}\")\n",
    "    exibir_colunas(df_2s, \"Segundo Semestre\")\n",
    "    \n",
    "    # Verifica√ß√£o de colunas iguais\n",
    "    colunas_1s = set(df_1s.columns)\n",
    "    colunas_2s = set(df_2s.columns)\n",
    "    \n",
    "    if colunas_1s == colunas_2s:\n",
    "        print(\"\\n‚úÖ Todas as colunas s√£o iguais entre os dois semestres.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå As colunas s√£o diferentes entre os semestres.\")\n",
    "        \n",
    "        # Mostrar diferen√ßas\n",
    "        apenas_1s = colunas_1s - colunas_2s\n",
    "        apenas_2s = colunas_2s - colunas_1s\n",
    "        \n",
    "        if apenas_1s:\n",
    "            print(f\"\\nüîç Colunas apenas no 1¬∫ semestre ({len(apenas_1s)}):\")\n",
    "            for col in sorted(apenas_1s):\n",
    "                print(f\"   - {col}\")\n",
    "                \n",
    "        if apenas_2s:\n",
    "            print(f\"\\nüîç Colunas apenas no 2¬∫ semestre ({len(apenas_2s)}):\")\n",
    "            for col in sorted(apenas_2s):\n",
    "                print(f\"   - {col}\")\n",
    "    \n",
    "    # Armazenando os DataFrames\n",
    "    dados_2011 = {\n",
    "        'primeiro_semestre': df_1s,\n",
    "        'segundo_semestre': df_2s\n",
    "    }\n",
    "    \n",
    "except UnicodeDecodeError:\n",
    "    print(\"\\n‚ùå Erro de codifica√ß√£o: O arquivo n√£o est√° em UTF-8. Tentando com ISO-8859-1...\")\n",
    "    try:\n",
    "        # Tentar novamente com ISO-8859-1 se UTF-8 falhar\n",
    "        df_1s = pd.read_csv(caminho_1s, encoding='ISO-8859-1', sep=';', decimal=',')\n",
    "        df_2s = pd.read_csv(caminho_2s, encoding='ISO-8859-1', sep=';', decimal=',')\n",
    "        print(\"‚úÖ Arquivos carregados com sucesso usando ISO-8859-1\")\n",
    "        \n",
    "        # Exibir informa√ß√µes novamente\n",
    "        exibir_colunas(df_1s, \"Primeiro Semestre (ISO-8859-1)\")\n",
    "        exibir_colunas(df_2s, \"Segundo Semestre (ISO-8859-1)\")\n",
    "        \n",
    "        # Armazenar os DataFrames\n",
    "        dados_2011 = {\n",
    "            'primeiro_semestre': df_1s,\n",
    "            'segundo_semestre': df_2s\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro ao carregar os arquivos: {str(e)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erro ao processar os arquivos: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_colunas(df, nome_periodo):\n",
    "    print(f\"\\nüîç An√°lise de Colunas - {nome_periodo}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Criar DataFrame com m√©tricas de cada coluna\n",
    "    analise = pd.DataFrame({\n",
    "        'Tipo': df.dtypes,\n",
    "        'Valores_√önicos': df.nunique(),\n",
    "        'Valores_Nulos': df.isnull().sum(),\n",
    "        '%_Nulos': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    # Classificar colunas\n",
    "    def classificar_coluna(linha):\n",
    "        # Colunas com alta cardinalidade ou muitos nulos s√£o menos √∫teis\n",
    "        if linha['%_Nulos'] > 50:\n",
    "            return '‚ùå Baixa Utilidade'\n",
    "        elif linha['Valores_√önicos'] == len(df):\n",
    "            return '‚ö†Ô∏è Identificador √önico (CNPJ, ID, etc)'\n",
    "        elif linha['Tipo'] == 'object':\n",
    "            if linha['Valores_√önicos'] < 50:\n",
    "                return '‚úÖ Categ√≥rica'\n",
    "            else:\n",
    "                return 'üìù Texto Livre'\n",
    "        elif 'data' in linha.name.lower():\n",
    "            return 'üìÖ Data (converter para datetime)'\n",
    "        elif 'valor' in linha.name.lower() or 'pre√ßo' in linha.name.lower():\n",
    "            return 'üí∞ M√©trica Num√©rica (importante)'\n",
    "        else:\n",
    "            return 'üî¢ Num√©rica'\n",
    "    \n",
    "    analise['Classifica√ß√£o'] = analise.apply(classificar_coluna, axis=1)\n",
    "    \n",
    "    # Ordenar por classifica√ß√£o e nome\n",
    "    analise = analise.sort_values(['Classifica√ß√£o', 'Valores_√önicos'])\n",
    "    \n",
    "    # Exibir resultados\n",
    "    with pd.option_context('display.max_rows', None, 'display.width', 1000):\n",
    "        display(analise)\n",
    "    \n",
    "    return analise\n",
    "\n",
    "# Analisar colunas de cada per√≠odo\n",
    "print(\"üîç INICIANDO AN√ÅLISE DAS COLUNAS\")\n",
    "analise_1s = analisar_colunas(df_1s, \"Primeiro Semestre\")\n",
    "analise_2s = analisar_colunas(df_2s, \"Segundo Semestre\")\n",
    "\n",
    "# Identificar diferen√ßas entre os per√≠odos\n",
    "colunas_diferentes = set(df_1s.columns).symmetric_difference(df_2s.columns)\n",
    "if colunas_diferentes:\n",
    "    print(\"\\n‚ö†Ô∏è  ATEN√á√ÉO: Diferen√ßas nas colunas entre os semestres:\", \n",
    "          ', '.join(colunas_diferentes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561f5bfcabbec4a",
   "metadata": {},
   "source": [
    "# üîç Sele√ß√£o Estrat√©gica de Colunas para An√°lise\n",
    "\n",
    "## üìä Colunas Selecionadas e Sua Relev√¢ncia\n",
    "\n",
    "### üìç Dados Geogr√°ficos\n",
    "- **Regi√£o**: Permite an√°lises macro por regi√£o do pa√≠s\n",
    "- **Estado**: Essencial para an√°lises estaduais e compara√ß√µes regionais\n",
    "- **Munic√≠pio**: Para an√°lises detalhadas em n√≠vel municipal\n",
    "- **Bandeira**: Importante para an√°lise de mercado e compara√ß√£o entre postos\n",
    "\n",
    "### üè∑Ô∏è Dados de Identifica√ß√£o\n",
    "- **Revenda**: Identifica o estabelecimento vendedor\n",
    "- **CNPJ da Revenda**: Identificador √∫nico para an√°lise por estabelecimento\n",
    "- **Produto**: Tipo de combust√≠vel (gasolina, diesel, etanol, etc)\n",
    "\n",
    "### üìÖ Dados Temporais\n",
    "- **Data da Coleta**: Fundamental para an√°lise de s√©ries temporais, tend√™ncias e sazonalidade\n",
    "\n",
    "### üí∞ Dados Financeiros\n",
    "- **Valor de Venda**: Pre√ßo final ao consumidor\n",
    "- **Valor de Compra**: Custo para a revenda\n",
    "- **Unidade de Medida**: Garante que as compara√ß√µes sejam feitas corretamente\n",
    "\n",
    "### üóëÔ∏è Colunas Removidas\n",
    "As demais colunas foram removidas por:\n",
    "- Conterem informa√ß√µes redundantes\n",
    "- N√£o serem relevantes para as an√°lises propostas\n",
    "- Estarem vazias ou com baixa taxa de preenchimento\n",
    "- N√£o adicionarem valor anal√≠tico significativo\n",
    "\n",
    "## üéØ Objetivo\n",
    "Esta sele√ß√£o visa otimizar o conjunto de dados para an√°lises de pre√ßos de combust√≠veis, mantendo apenas as informa√ß√µes mais relevantes para:\n",
    "- An√°lises de pre√ßos por regi√£o/estado/munic√≠pio\n",
    "- Compara√ß√£o entre bandeiras de postos\n",
    "- An√°lises temporais de varia√ß√£o de pre√ßos\n",
    "- C√°lculo de margens de lucro (diferen√ßa entre venda e compra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7bc97c9b32dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicion√°rio de padroniza√ß√£o dos nomes das colunas\n",
    "padrao_colunas = {\n",
    "    'Regiao - Sigla': 'regiao_sigla',\n",
    "    'Estado - Sigla': 'estado_sigla',\n",
    "    'Municipio': 'municipio',\n",
    "    'Revenda': 'revenda',\n",
    "    'CNPJ da Revenda': 'cnpj_revenda',\n",
    "    'Produto': 'produto',\n",
    "    'Data da Coleta': 'data_coleta',\n",
    "    'Valor de Venda': 'valor_venda',\n",
    "    'Valor de Compra': 'valor_compra',\n",
    "    'Unidade de Medida': 'unidade_medida',\n",
    "    'Bandeira': 'bandeira'\n",
    "}\n",
    "\n",
    "# Lista das colunas que ser√£o mantidas (usando os nomes originais)\n",
    "colunas_essenciais = [\n",
    "    'Regiao - Sigla',\n",
    "    'Estado - Sigla',\n",
    "    'Municipio',\n",
    "    'Revenda',\n",
    "    'CNPJ da Revenda',\n",
    "    'Produto',\n",
    "    'Data da Coleta',\n",
    "    'Valor de Venda',\n",
    "    'Valor de Compra',\n",
    "    'Unidade de Medida',\n",
    "    'Bandeira'\n",
    "]\n",
    "\n",
    "# Filtrar e renomear as colunas\n",
    "def preparar_dataframe(df):\n",
    "    # Filtrar colunas\n",
    "    df = df[colunas_essenciais].copy()\n",
    "    # Renomear colunas\n",
    "    return df.rename(columns=padrao_colunas)\n",
    "\n",
    "# Aplicar a ambos os dataframes\n",
    "df_1s = preparar_dataframe(df_1s)\n",
    "df_2s = preparar_dataframe(df_2s)\n",
    "\n",
    "# Verificar o resultado\n",
    "print(\"üîç Colunas ap√≥s a filtragem e padroniza√ß√£o:\")\n",
    "print(\"-\" * 50)\n",
    "print(df_1s.columns.tolist())\n",
    "\n",
    "# Mostrar as primeiras linhas do primeiro semestre como exemplo\n",
    "print(\"\\nüìã Dados do Primeiro Semestre (amostra):\")\n",
    "display(df_1s.head(2))\n",
    "\n",
    "# Mostrar as primeiras linhas do segundo semestre como exemplo\n",
    "print(\"\\nüìã Dados do Segundo Semestre (amostra):\")\n",
    "display(df_2s.head(2))\n",
    "\n",
    "# Converter a coluna de data\n",
    "df_1s['data_coleta'] = pd.to_datetime(df_1s['data_coleta'], dayfirst=True)\n",
    "df_2s['data_coleta'] = pd.to_datetime(df_2s['data_coleta'], dayfirst=True)\n",
    "\n",
    "print(\"\\n‚úÖ Dados preparados com sucesso!\")\n",
    "print(f\"Primeiro semestre: {len(df_1s)} registros\")\n",
    "print(f\"Segundo semestre: {len(df_2s)} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bd5e4",
   "metadata": {},
   "source": [
    "# üîç An√°lise de Valores Nulos\n",
    "\n",
    "## üìä Vis√£o Geral dos Dados Ausentes\n",
    "\n",
    "Vamos analisar a quantidade e porcentagem de valores nulos em cada coluna do nosso conjunto de dados. Esta an√°lise √© crucial para entendermos a qualidade dos dados e decidirmos as melhores estrat√©gias de tratamento.\n",
    "\n",
    "### Por que essa an√°lise √© importante?\n",
    "1. **Qualidade dos Dados**: Identificar colunas com muitos valores ausentes que podem comprometer as an√°lises.\n",
    "2. **Tomada de Decis√£o**: Decidir se devemos preencher, remover ou manter os valores nulos.\n",
    "3. **Impacto nas An√°lises**: Entender como os valores ausentes podem afetar os resultados das nossas an√°lises.\n",
    "\n",
    "### Pr√≥ximos Passos:\n",
    "1. Verificar a quantidade de valores nulos por coluna\n",
    "2. Calcular a porcentagem de valores nulos em rela√ß√£o ao total de registros\n",
    "3. Identificar padr√µes nos dados ausentes\n",
    "4. Decidir a melhor estrat√©gia de tratamento para cada caso\n",
    "\n",
    "Vamos come√ßar executando a an√°lise de valores nulos para cada semestre separadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26090483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para an√°lise de valores nulos\n",
    "def analisar_nulos(df, nome_periodo):\n",
    "    print(f\"\\nüîç An√°lise de Valores Nulos - {nome_periodo}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calcular totais\n",
    "    total_registros = len(df)\n",
    "    print(f\"üìä Total de registros: {total_registros:,}\")\n",
    "    \n",
    "    # Criar DataFrame com an√°lise de nulos\n",
    "    nulos = pd.DataFrame({\n",
    "        'Valores_Nulos': df.isnull().sum(),\n",
    "        'Percentual_Nulos': (df.isnull().sum() / total_registros * 100).round(2)\n",
    "    }).sort_values('Valores_Nulos', ascending=False)\n",
    "    \n",
    "    # Exibir apenas colunas com valores nulos\n",
    "    nulos = nulos[nulos['Valores_Nulos'] > 0]\n",
    "    \n",
    "    if len(nulos) == 0:\n",
    "        print(\"‚úÖ Nenhum valor nulo encontrado!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Colunas com valores nulos: {len(nulos)} de {len(df.columns)} colunas\")\n",
    "        display(nulos)\n",
    "    \n",
    "    return nulos\n",
    "\n",
    "# Executar an√°lise para ambos os semestres\n",
    "nulos_1s = analisar_nulos(df_1s, \"Primeiro Semestre\")\n",
    "nulos_2s = analisar_nulos(df_2s, \"Segundo Semestre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b38b1a",
   "metadata": {},
   "source": [
    "# üßπ Tratamento de Valores Ausentes\n",
    "\n",
    "## üìä Estrat√©gia de Preenchimento\n",
    "\n",
    "### Para Valores Num√©ricos (ex: Valor de Compra):\n",
    "- **M√©todo**: Preenchimento com a mediana do grupo\n",
    "- **Vantagens**:\n",
    "  - Menos sens√≠vel a outliers que a m√©dia\n",
    "  - Mant√©m a distribui√ß√£o original dos dados\n",
    "  - Considera o contexto (produto, bandeira, regi√£o)\n",
    "\n",
    "### Para Valores Categ√≥ricos (ex: Bandeira, Munic√≠pio):\n",
    "- **M√©todo**: Preenchimento com categoria \"Desconhecido\"\n",
    "- **Vantagens**:\n",
    "  - N√£o inventa dados que n√£o existem\n",
    "  - Mant√©m a transpar√™ncia do tratamento\n",
    "  - Permite identificar registros que precisam de aten√ß√£o especial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputar_valores_numericos(df, coluna, colunas_agrupar):\n",
    "    \"\"\"\n",
    "    Imputa valores num√©ricos faltantes usando a mediana do grupo.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - df: DataFrame com os dados\n",
    "    - coluna: Nome da coluna num√©rica a ser preenchida\n",
    "    - colunas_agrupar: Lista de colunas para agrupar e calcular a mediana\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame com os valores faltantes preenchidos\n",
    "    \"\"\"\n",
    "    # Criar c√≥pia para evitar avisos\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calcular a mediana por grupo\n",
    "    medianas = df.groupby(colunas_agrupar)[coluna].transform('median')\n",
    "    \n",
    "    # Preencher valores nulos com a mediana do grupo\n",
    "    df[coluna] = df[coluna].fillna(medianas)\n",
    "    \n",
    "    # Se ainda houver valores nulos (grupos sem amostras), preencher com a mediana global\n",
    "    if df[coluna].isnull().any():\n",
    "        mediana_global = df[coluna].median()\n",
    "        df[coluna] = df[coluna].fillna(mediana_global)\n",
    "        print(f\"  - {df[coluna].isnull().sum()} valores nulos restantes em '{coluna}' preenchidos com mediana global\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def imputar_valores_categoricos(df, colunas):\n",
    "    \"\"\"\n",
    "    Preenche valores categ√≥ricos faltantes com a moda (valor mais frequente) de cada coluna.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - df: DataFrame com os dados\n",
    "    - colunas: Lista de colunas categ√≥ricas a serem preenchidas\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame com os valores faltantes preenchidos\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in colunas:\n",
    "        if col in df.columns:\n",
    "            # Encontrar o valor mais frequente (moda)\n",
    "            moda = df[col].mode()[0]\n",
    "            # Contar valores nulos antes\n",
    "            nulos_antes = df[col].isnull().sum()\n",
    "            # Preencher valores nulos\n",
    "            df[col] = df[col].fillna(moda)\n",
    "            # Contar valores nulos depois\n",
    "            nulos_depois = df[col].isnull().sum()\n",
    "            \n",
    "            if nulos_antes > 0:\n",
    "                print(f\"  - {nulos_antes} valores nulos em '{col}' preenchidos com '{moda}'\")\n",
    "    return df\n",
    "\n",
    "# Para o primeiro semestre\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä TRATAMENTO - PRIMEIRO SEMESTRE\".center(60))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tratar valores num√©ricos\n",
    "colunas_agrupar = ['produto', 'estado_sigla', 'municipio']\n",
    "df_1s = imputar_valores_numericos(df_1s, 'valor_venda', colunas_agrupar)\n",
    "df_1s = imputar_valores_numericos(df_1s, 'valor_compra', colunas_agrupar)\n",
    "\n",
    "# Tratar valores categ√≥ricos\n",
    "colunas_cat = ['bandeira', 'unidade_medida', 'regiao_sigla']\n",
    "df_1s = imputar_valores_categoricos(df_1s, colunas_cat)\n",
    "\n",
    "# Para o segundo semestre\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä TRATAMENTO - SEGUNDO SEMESTRE\".center(60))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tratar valores num√©ricos\n",
    "df_2s = imputar_valores_numericos(df_2s, 'valor_venda', colunas_agrupar)\n",
    "df_2s = imputar_valores_numericos(df_2s, 'valor_compra', colunas_agrupar)\n",
    "\n",
    "# Tratar valores categ√≥ricos\n",
    "df_2s = imputar_valores_categoricos(df_2s, colunas_cat)\n",
    "\n",
    "print(\"\\n‚úÖ Tratamento conclu√≠do para ambos os semestres!\")\n",
    "print(f\"- Primeiro semestre: {len(df_1s)} registros\")\n",
    "print(f\"- Segundo semestre: {len(df_2s)} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se ainda existem valores nulos\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFICA√á√ÉO FINAL DE VALORES NULOS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPrimeiro Semestre:\")\n",
    "display(df_1s.isnull().sum())\n",
    "print(\"\\nSegundo Semestre:\")\n",
    "display(df_2s.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b56f9e",
   "metadata": {},
   "source": [
    "# üîç An√°lise de Inconsist√™ncias nos Pre√ßos\n",
    "\n",
    "## üìå Objetivo\n",
    "Identificar e corrigir registros onde `Valor de Venda < Valor de Compra`, o que indica preju√≠zo financeiro.\n",
    "\n",
    "## üí° Por que analisar?\n",
    "- **Erros de Dados**: Identificar registros com valores incorretos\n",
    "- **Sa√∫de Financeira**: Garantir margens positivas\n",
    "- **Qualidade**: Manter a confiabilidade do dataset\n",
    "\n",
    "## üìä O que vamos verificar?\n",
    "- Quantos registros est√£o com preju√≠zo\n",
    "- Quais produtos/bandeiras t√™m mais problemas\n",
    "- Qual o impacto financeiro dessas inconsist√™ncias\n",
    "\n",
    "## üîß Pr√≥ximos Passos\n",
    "1. Identificar registros problem√°ticos\n",
    "2. Analisar padr√µes\n",
    "3. Corrigir os valores\n",
    "4. Validar os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_inconsistencias_precos(df, nome_periodo):\n",
    "    \"\"\"\n",
    "    Verifica se existem valores de venda menores que os de compra,\n",
    "    o que seria uma inconsist√™ncia nos dados.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - df: DataFrame com os dados\n",
    "    - nome_periodo: Nome do per√≠odo para exibi√ß√£o\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame com as inconsist√™ncias encontradas ou None se n√£o houver\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Verificando Inconsist√™ncias de Pre√ßos - {nome_periodo}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Verificar se existem valores de venda menores que compra\n",
    "    inconsistencias = df[df['valor_venda'] < df['valor_compra']].copy()\n",
    "    total_inconsistencias = len(inconsistencias)\n",
    "    total_linhas = len(df)\n",
    "    percentual = (total_inconsistencias / total_linhas) * 100 if total_linhas > 0 else 0\n",
    "    \n",
    "    # Resumo Executivo\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä RESUMO DAS INCONSIST√äNCIAS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"‚úÖ Total de registros analisados: {total_linhas:,}\")\n",
    "    print(f\"‚ùå Registros com VENDA < COMPRA: {total_inconsistencias:,} ({percentual:.2f}%)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if total_inconsistencias > 0:\n",
    "        # Calcular a diferen√ßa\n",
    "        inconsistencias['diferenca'] = inconsistencias['valor_compra'] - inconsistencias['valor_venda']\n",
    "        impacto_total = inconsistencias['diferenca'].sum()\n",
    "        \n",
    "        print(\"\\nüìã DETALHES DAS INCONSIST√äNCIAS:\")\n",
    "        print(f\"  - Diferen√ßa M√©dia: R$ {inconsistencias['diferenca'].mean():.2f}\")\n",
    "        print(f\"  - Maior Diferen√ßa: R$ {inconsistencias['diferenca'].max():.2f}\")\n",
    "        print(f\"  - Menor Diferen√ßa: R$ {inconsistencias['diferenca'].min():.2f}\")\n",
    "        print(f\"  - Impacto Total:  R$ {impacto_total:,.2f}\")\n",
    "        \n",
    "        print(\"\\nüìä AMOSTRA DAS INCONSIST√äNCIAS (Top 5):\")\n",
    "        display(inconsistencias[['produto', 'bandeira', 'valor_compra', \n",
    "                              'valor_venda', 'diferenca']]\n",
    "               .sort_values('diferenca', ascending=False).head())\n",
    "        \n",
    "        # An√°lise por produto\n",
    "        print(\"\\nüìà AN√ÅLISE POR PRODUTO:\")\n",
    "        analise_produto = inconsistencias.groupby('produto').agg(\n",
    "            qtd_inconsistencias=('produto', 'size'),\n",
    "            media_diferenca=('diferenca', 'mean'),\n",
    "            impacto_total=('diferenca', 'sum')\n",
    "        ).sort_values('qtd_inconsistencias', ascending=False)\n",
    "        display(analise_produto)\n",
    "        \n",
    "        return inconsistencias\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Nenhuma inconsist√™ncia encontrada!\")\n",
    "        return None\n",
    "\n",
    "# Executar verifica√ß√£o para ambos os semestres\n",
    "print(\"=\"*70)\n",
    "print(\"AN√ÅLISE DE INCONSIST√äNCIAS NOS PRE√áOS (venda < compra)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar primeiro semestre\n",
    "inconsistencias_1s = verificar_inconsistencias_precos(df_1s, \"Primeiro Semestre\")\n",
    "\n",
    "# Verificar segundo semestre\n",
    "inconsistencias_2s = verificar_inconsistencias_precos(df_2s, \"Segundo Semestre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0f201",
   "metadata": {},
   "source": [
    "# üîÑ Corre√ß√£o de Inconsist√™ncias de Pre√ßos\n",
    "\n",
    "## üìä Estrat√©gia de Corre√ß√£o\n",
    "\n",
    "### Para Vendas com Preju√≠zo (Venda < Compra):\n",
    "- **M√©todo**: Ajuste com margem de seguran√ßa\n",
    "- **F√≥rmula**:  \n",
    "  `Novo Valor de Venda = Valor de Compra √ó (1 + margem_seguranca)`\n",
    "\n",
    "- **Vantagens**:\n",
    "  - Elimina preju√≠zos nas vendas\n",
    "  - Mant√©m margem de lucro m√≠nima configur√°vel\n",
    "  - Aplica corre√ß√£o apenas onde necess√°rio\n",
    "\n",
    "### Para Vendas V√°lidas (Venda ‚â• Compra):\n",
    "- **M√©todo**: Mant√©m os valores originais\n",
    "- **Vantagens**:\n",
    "  - Preserva os dados corretos\n",
    "  - N√£o altera registros que j√° est√£o consistentes\n",
    "  - Mant√©m a integridade das informa√ß√µes\n",
    "\n",
    "## ‚öôÔ∏è Configura√ß√£o Padr√£o\n",
    "- **Margem de Seguran√ßa**: 10% (0.10)\n",
    "- **Personaliza√ß√£o**: Ajust√°vel conforme necessidade\n",
    "- **N√£o-destrutivo**: Nunca altera o DataFrame original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrigir_inconsistencias(df, margem_seguranca=0.10, nome_conjunto=\"Dados\"):\n",
    "    \"\"\"\n",
    "    Corrige automaticamente as inconsist√™ncias nos pre√ßos,\n",
    "    garantindo que valor_venda seja sempre maior que valor_compra.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - df: DataFrame com os dados\n",
    "    - margem_seguranca: percentual a ser adicionado ao valor de compra (padr√£o: 10%)\n",
    "    - nome_conjunto: Nome do conjunto de dados para refer√™ncia\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame corrigido\n",
    "    \"\"\"\n",
    "    # Verificar se as colunas existem\n",
    "    colunas_necessarias = ['valor_venda', 'valor_compra']\n",
    "    for col in colunas_necessarias:\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"Coluna '{col}' n√£o encontrada no DataFrame. Colunas dispon√≠veis: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Identificar linhas com problemas\n",
    "    mask = df['valor_venda'] < df['valor_compra']\n",
    "    total_corrigir = mask.sum()\n",
    "    \n",
    "    if total_corrigir > 0:\n",
    "        print(f\"\\nüîß CORRIGINDO {total_corrigir:,} REGISTROS - {nome_conjunto.upper()}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Aplicar corre√ß√£o\n",
    "        df_corrigido = df.copy()\n",
    "        df_corrigido.loc[mask, 'valor_venda'] = (\n",
    "            df_corrigido.loc[mask, 'valor_compra'] * (1 + margem_seguranca)\n",
    "        ).round(4)  # Arredonda para 4 casas decimais\n",
    "        \n",
    "        print(f\"‚úÖ Corre√ß√£o aplicada com sucesso! (Margem: {margem_seguranca*100:.0f}%)\")\n",
    "        print(\"=\"*60)\n",
    "        return df_corrigido\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Nenhuma corre√ß√£o necess√°ria em {nome_conjunto}!\")\n",
    "        return df.copy()\n",
    "\n",
    "# =============================================\n",
    "# EXECU√á√ÉO DAS CORRE√á√ïES\n",
    "# =============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÑ INICIANDO PROCESSO DE CORRE√á√ÉO\".center(60))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verificar colunas dispon√≠veis antes de executar\n",
    "print(\"\\nüìã Colunas dispon√≠veis no df_1s:\", df_1s.columns.tolist())\n",
    "print(\"üìã Colunas dispon√≠veis no df_2s:\", df_2s.columns.tolist())\n",
    "\n",
    "# Primeiro Semestre\n",
    "print(\"\\n\" + \"=\"*30 + \" PRIMEIRO SEMESTRE \" + \"=\"*30)\n",
    "df_1s_corrigido = corrigir_inconsistencias(df_1s, nome_conjunto=\"Primeiro Semestre\")\n",
    "\n",
    "# Segundo Semestre\n",
    "print(\"\\n\" + \"=\"*30 + \" SEGUNDO SEMESTRE \" + \"=\"*31)\n",
    "df_2s_corrigido = corrigir_inconsistencias(df_2s, nome_conjunto=\"Segundo Semestre\")\n",
    "\n",
    "# Juntar os dados corrigidos\n",
    "df_total_corrigido = pd.concat([df_1s_corrigido, df_2s_corrigido])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ CORRE√á√ïES CONCLU√çDAS COM SUCESSO!\".center(60))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verifica√ß√£o final\n",
    "print(\"\\nüìä RESUMO DAS CORRE√á√ïES:\")\n",
    "print(f\"- Total de registros processados: {len(df_total_corrigido):,}\")\n",
    "print(f\"- Registros no 1¬∫ semestre: {len(df_1s_corrigido):,}\")\n",
    "print(f\"- Registros no 2¬∫ semestre: {len(df_2s_corrigido):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae6bfb",
   "metadata": {},
   "source": [
    "# üìÖ Tratamento de Datas\n",
    "\n",
    "## üîç Objetivo\n",
    "Padronizar e enriquecer as informa√ß√µes temporais para permitir an√°lises temporais mais precisas e completas.\n",
    "\n",
    "## üõ†Ô∏è Estrat√©gia de Processamento\n",
    "\n",
    "### 1. Convers√£o para Datetime\n",
    "- **M√©todo**: Convers√£o para o tipo `datetime64[ns]`\n",
    "- **Tratamento de erros**: \n",
    "  - Valores inv√°lidos convertidos para `NaT` (Not a Time)\n",
    "  - Formato preferencial: `DD/MM/YYYY`\n",
    "- **Vantagens**:\n",
    "  - Opera√ß√µes de data mais eficientes\n",
    "  - Suporte a ordena√ß√£o e filtros temporais\n",
    "\n",
    "### 2. Extra√ß√£o de Componentes Temporais\n",
    "- **Componentes extra√≠dos**:\n",
    "  - `data_ano`: Ano (ex: 2025)\n",
    "  - `data_mes`: M√™s num√©rico (1-12)\n",
    "  - `data_dia`: Dia do m√™s (1-31)\n",
    "  - `data_dia_semana`: Nome do dia (Segunda a Domingo)\n",
    "  - `data_trimestre`: Trimestre (1-4)\n",
    "  - `data_semana_ano`: N√∫mero da semana (1-52)\n",
    "  - `data_ano_mes`: Per√≠odo no formato 'YYYY-MM'\n",
    "\n",
    "### 3. Valida√ß√£o de Consist√™ncia\n",
    "- **Verifica√ß√µes**:\n",
    "  - Datas futuras em rela√ß√£o √† data de processamento\n",
    "  - Valores nulos antes/depois da convers√£o\n",
    "  - Per√≠odos fora do esperado\n",
    "- **A√ß√µes**:\n",
    "  - Registros com datas inv√°lidas s√£o marcados\n",
    "  - Relat√≥rio de inconsist√™ncias gerado\n",
    "\n",
    "## üìà Benef√≠cios\n",
    "- An√°lises temporais mais ricas\n",
    "- Facilidade de agrega√ß√£o por per√≠odos\n",
    "- Identifica√ß√£o de sazonalidades\n",
    "- Suporte a an√°lises de tend√™ncias temporais\n",
    "\n",
    "## ‚ö†Ô∏è Considera√ß√µes\n",
    "- Datas fora do per√≠odo esperado s√£o mantidas, mas sinalizadas\n",
    "- O formato original da data √© preservado na coluna original\n",
    "- Novas colunas s√£o adicionadas ao final do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e48acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_e_separar_datas(df, coluna_data, nome_conjunto=\"Dados\"):\n",
    "    \"\"\"\n",
    "    Verifica e separa uma coluna de data em componentes individuais.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - df: DataFrame com os dados\n",
    "    - coluna_data: Nome da coluna de data a ser processada\n",
    "    - nome_conjunto: Nome do conjunto de dados para refer√™ncia\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame com as novas colunas de data adicionadas\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìÖ PROCESSAMENTO DE DATAS - {nome_conjunto.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Verificar se a coluna existe\n",
    "    if coluna_data not in df.columns:\n",
    "        print(f\"‚ùå Erro: A coluna '{coluna_data}' n√£o foi encontrada no DataFrame.\")\n",
    "        print(\"   Colunas dispon√≠veis:\", df.columns.tolist())\n",
    "        return df\n",
    "    \n",
    "    # Fazer uma c√≥pia para n√£o modificar o original\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # 1. Verificar valores nulos\n",
    "    nulos_antes = df_temp[coluna_data].isnull().sum()\n",
    "    total = len(df_temp)\n",
    "    print(f\"\\nüîç An√°lise da coluna '{coluna_data}':\")\n",
    "    print(f\"   - Total de registros: {total:,}\")\n",
    "    print(f\"   - Valores nulos: {nulos_antes:,} ({nulos_antes/total*100:.2f}%)\")\n",
    "    \n",
    "    # 2. Converter para datetime\n",
    "    try:\n",
    "        df_temp[coluna_data] = pd.to_datetime(df_temp[coluna_data], errors='coerce', dayfirst=True)\n",
    "        \n",
    "        # Verificar convers√µes que falharam\n",
    "        nulos_depois = df_temp[coluna_data].isnull().sum()\n",
    "        if nulos_depois > nulos_antes:\n",
    "            print(f\"‚ö†Ô∏è  {nulos_depois - nulos_antes:,} valores n√£o puderam ser convertidos para data\")\n",
    "        \n",
    "        # 3. Extrair componentes da data\n",
    "        print(\"\\nüìÖ Criando novas colunas de data:\")\n",
    "        componentes = {\n",
    "            f'data_ano': df_temp[coluna_data].dt.year,\n",
    "            f'data_mes': df_temp[coluna_data].dt.month,\n",
    "            f'data_dia': df_temp[coluna_data].dt.day,\n",
    "            f'data_dia_semana': df_temp[coluna_data].dt.day_name(),\n",
    "            f'data_mes_nome': df_temp[coluna_data].dt.month_name(),\n",
    "            f'data_trimestre': df_temp[coluna_data].dt.quarter,\n",
    "            f'data_semana_ano': df_temp[coluna_data].dt.isocalendar().week,\n",
    "            f'data_ano_mes': df_temp[coluna_data].dt.to_period('M').astype(str)\n",
    "        }\n",
    "        \n",
    "        # Adicionar ao DataFrame\n",
    "        for nome, valores in componentes.items():\n",
    "            df_temp[nome] = valores\n",
    "            print(f\"   - Adicionada coluna: {nome}\")\n",
    "        \n",
    "        # 4. Verificar estat√≠sticas\n",
    "        print(\"\\nüìä Estat√≠sticas das datas:\")\n",
    "        print(f\"   - Data mais antiga: {df_temp[coluna_data].min()}\")\n",
    "        print(f\"   - Data mais recente: {df_temp[coluna_data].max()}\")\n",
    "        print(f\"   - Per√≠odo coberto: {(df_temp[coluna_data].max() - df_temp[coluna_data].min()).days} dias\")\n",
    "        \n",
    "        # Verificar datas futuras\n",
    "        hoje = pd.Timestamp('today')\n",
    "        datas_futuras = (df_temp[coluna_data] > hoje).sum()\n",
    "        if datas_futuras > 0:\n",
    "            print(f\"‚ö†Ô∏è  {datas_futuras:,} datas futuras detectadas\")\n",
    "        \n",
    "        # Verificar distribui√ß√£o por ano-m√™s\n",
    "        print(\"\\nüìà Distribui√ß√£o por ano-m√™s:\")\n",
    "        dist_ano_mes = df_temp['data_ano_mes'].value_counts().sort_index()\n",
    "        print(dist_ano_mes)\n",
    "        \n",
    "        print(\"\\n‚úÖ Processamento de datas conclu√≠do com sucesso!\")\n",
    "        return df_temp\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro ao processar datas: {str(e)}\")\n",
    "        return df\n",
    "\n",
    "# Exemplo de uso:\n",
    "# Para o primeiro semestre\n",
    "print(\"=\"*70)\n",
    "print(\"PRIMEIRO SEMESTRE\".center(70))\n",
    "print(\"=\"*70)\n",
    "df_1s = verificar_e_separar_datas(df_1s, 'data_coleta', \"Primeiro Semestre\")\n",
    "\n",
    "# Para o segundo semestre\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SEGUNDO SEMESTRE\".center(70))\n",
    "print(\"=\"*70)\n",
    "df_2s = verificar_e_separar_datas(df_2s, 'data_coleta', \"Segundo Semestre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843339ee",
   "metadata": {},
   "source": [
    "# üîç An√°lise de Valores √önicos\n",
    "\n",
    "## üìä Estrat√©gia de An√°lise\n",
    "\n",
    "### Para Colunas com Baixa Cardinalidade (At√© 20 valores √∫nicos):\n",
    "- **M√©todo**: Listagem completa de valores\n",
    "- **A√ß√£o**: \n",
    "  - Exibe todos os valores distintos\n",
    "  - Sugere convers√£o para tipo `category` quando aplic√°vel\n",
    "- **Vantagens**:\n",
    "  - Identifica rapidamente categorias e op√ß√µes\n",
    "  - Facilita a detec√ß√£o de erros de digita√ß√£o\n",
    "  - Otimiza o uso de mem√≥ria\n",
    "\n",
    "### Para Colunas com M√©dia/Alta Cardinalidade (Acima de 20 valores √∫nicos):\n",
    "- **M√©todo**: Amostragem dos valores\n",
    "- **A√ß√£o**:\n",
    "  - Exibe os 5 primeiros valores\n",
    "  - Informa o total de valores √∫nicos\n",
    "  - Calcula a taxa de cardinalidade\n",
    "- **Vantagens**:\n",
    "  - Fornece visibilidade sem sobrecarregar a sa√≠da\n",
    "  - Permite identificar padr√µes e outliers\n",
    "  - Facilita a detec√ß√£o de problemas de qualidade\n",
    "\n",
    "## ‚öôÔ∏è M√©tricas Calculadas\n",
    "- **Total de Valores √önicos**: Contagem de valores distintos\n",
    "- **Taxa de Cardinalidade**: `(Valores √önicos / Total de Registros) √ó 100`\n",
    "- **Tipo de Dados**: Tipo primitivo da coluna (int, float, object, etc.)\n",
    "- **Amostra de Valores**: Primeiros 5 valores ou todos, dependendo da cardinalidade\n",
    "\n",
    "## üìå Sa√≠da\n",
    "- Formato padronizado para f√°cil leitura\n",
    "- Destaque visual para colunas problem√°ticas\n",
    "- Sugest√µes de tratamento quando aplic√°vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a524f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_valores_unicos(df, nome_df):\n",
    "    \"\"\"\n",
    "    üîç Analisa e exibe os valores √∫nicos de cada coluna do DataFrame.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - df: DataFrame a ser analisado\n",
    "    - nome_df: Nome do DataFrame para exibi√ß√£o\n",
    "    \n",
    "    Sa√≠da:\n",
    "    - Para cada coluna:\n",
    "      - Nome e tipo de dados\n",
    "      - Contagem de valores √∫nicos e totais\n",
    "      - Amostra dos valores (completa ou parcial)\n",
    "      - Estat√≠sticas para colunas num√©ricas\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç AN√ÅLISE DE VALORES √öNICOS - {nome_df.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for coluna in df.columns:\n",
    "        # Informa√ß√µes b√°sicas\n",
    "        valores_unicos = df[coluna].unique()\n",
    "        num_valores = len(valores_unicos)\n",
    "        total_registros = len(df[coluna])\n",
    "        percentual = (num_valores / total_registros) * 100\n",
    "        \n",
    "        print(f\"\\nüìå {coluna} ({df[coluna].dtype})\")\n",
    "        print(f\"   - Valores √∫nicos: {num_valores} de {total_registros} ({percentual:.1f}%)\")\n",
    "        \n",
    "        # Exibi√ß√£o dos valores\n",
    "        if num_valores <= 10:\n",
    "            print(\"   - Valores:\", valores_unicos)\n",
    "        else:\n",
    "            print(f\"   - Amostra (5/{num_valores}): {valores_unicos[:5]}...\")\n",
    "        \n",
    "        # Estat√≠sticas para colunas num√©ricas\n",
    "        if pd.api.types.is_numeric_dtype(df[coluna]):\n",
    "            print(f\"   - Estat√≠sticas: Min={df[coluna].min():.2f} | \"\n",
    "                  f\"Max={df[coluna].max():.2f} | \"\n",
    "                  f\"M√©dia={df[coluna].mean():.2f}\")\n",
    "        \n",
    "        # Sugest√µes baseadas na cardinalidade\n",
    "        if num_valores <= 20:\n",
    "            print(\"   üí° Sugest√£o: Boa candidata para convers√£o a 'category'\")\n",
    "        elif num_valores == total_registros:\n",
    "            print(\"   ‚ö†Ô∏è  Poss√≠vel chave √∫nica ou identificador\")\n",
    "        \n",
    "        print(\"-\"*50)\n",
    "\n",
    "# Executar a an√°lise\n",
    "print(\"=\"*70)\n",
    "print(\"üîç INICIANDO AN√ÅLISE DE VALORES √öNICOS\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar valores √∫nicos para ambos os semestres\n",
    "verificar_valores_unicos(df_1s, \"Primeiro Semestre\")\n",
    "verificar_valores_unicos(df_2s, \"Segundo Semestre\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lise conclu√≠da com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f41c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e169d7c",
   "metadata": {},
   "source": [
    "# üîÑ Padroniza√ß√£o de Dados\n",
    "\n",
    "## üìä Estrat√©gia de Padroniza√ß√£o\n",
    "\n",
    "### Para Colunas de Texto (munic√≠pios, revendas, bandeiras):\n",
    "- **M√©todo**: Convers√£o para t√≠tulo\n",
    "- **A√ß√£o**:\n",
    "  - Converte para formato de t√≠tulo (primeira letra mai√∫scula)\n",
    "  - Remove espa√ßos extras\n",
    "- **Vantagens**:\n",
    "  - Padroniza√ß√£o visual consistente\n",
    "  - Facilita buscas e compara√ß√µes\n",
    "  - Melhora a legibilidade\n",
    "\n",
    "### Para Colunas de Identifica√ß√£o (CNPJ):\n",
    "- **M√©todo**: Limpeza de caracteres\n",
    "- **A√ß√£o**:\n",
    "  - Remove espa√ßos em branco\n",
    "  - Mant√©m apenas d√≠gitos e caracteres especiais de formata√ß√£o\n",
    "- **Vantagens**:\n",
    "  - Padroniza√ß√£o do formato\n",
    "  - Facilita valida√ß√µes\n",
    "  - Remove inconsist√™ncias\n",
    "\n",
    "### Para Datas e Valores Temporais:\n",
    "- **M√©todo**: Tradu√ß√£o e formata√ß√£o\n",
    "- **A√ß√£o**:\n",
    "  - Traduz dias da semana para portugu√™s\n",
    "  - Traduz meses para portugu√™s\n",
    "  - Mant√©m formato consistente\n",
    "- **Vantagens**:\n",
    "  - Padroniza√ß√£o lingu√≠stica\n",
    "  - Melhor compreens√£o\n",
    "  - Facilita an√°lises temporais\n",
    "\n",
    "### Para Colunas Categ√≥ricas:\n",
    "- **M√©todo**: Convers√£o para tipo `category`\n",
    "- **A√ß√£o**:\n",
    "  - Aplicado a colunas com baixa cardinalidade\n",
    "  - Mant√©m a ordem quando relevante\n",
    "- **Vantagens**:\n",
    "  - Redu√ß√£o no uso de mem√≥ria\n",
    "  - Melhor desempenho em opera√ß√µes\n",
    "  - Facilita agrupamentos\n",
    "\n",
    "## ‚öôÔ∏è M√©tricas de Qualidade\n",
    "- **Consist√™ncia**: Dados no mesmo formato\n",
    "- **Integridade**: Sem valores faltantes ap√≥s padroniza√ß√£o\n",
    "- **Legibilidade**: Nomes claros e padronizados\n",
    "- **Desempenho**: Uso otimizado de mem√≥ria\n",
    "\n",
    "## üìå Resultados Esperados\n",
    "- Dados consistentes e padronizados\n",
    "- Melhor desempenho em opera√ß√µes\n",
    "- Facilidade de manuten√ß√£o\n",
    "- Base pronta para an√°lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f415ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Padroniza√ß√£o de Texto (munic√≠pios)\n",
    "df_1s['municipio'] = df_1s['municipio'].str.title()\n",
    "df_2s['municipio'] = df_2s['municipio'].str.title()\n",
    "\n",
    "# 2. Padroniza√ß√£o de CNPJ\n",
    "df_1s['cnpj_revenda'] = df_1s['cnpj_revenda'].str.strip()\n",
    "df_2s['cnpj_revenda'] = df_2s['cnpj_revenda'].str.strip()\n",
    "\n",
    "# 3. Padroniza√ß√£o de Unidade de Medida\n",
    "df_1s['unidade_medida'] = df_1s['unidade_medida'].str.strip().str.upper()\n",
    "df_2s['unidade_medida'] = df_2s['unidade_medida'].str.strip().str.upper()\n",
    "\n",
    "# 4. Tradu√ß√£o de Dias da Semana\n",
    "dias_traducao = {\n",
    "    'Monday': 'Segunda', \n",
    "    'Tuesday': 'Ter√ßa', \n",
    "    'Wednesday': 'Quarta',\n",
    "    'Thursday': 'Quinta', \n",
    "    'Friday': 'Sexta', \n",
    "    'Saturday': 'S√°bado',\n",
    "    'Sunday': 'Domingo'\n",
    "}\n",
    "df_1s['data_dia_semana'] = df_1s['data_dia_semana'].map(dias_traducao)\n",
    "df_2s['data_dia_semana'] = df_2s['data_dia_semana'].map(dias_traducao)\n",
    "\n",
    "# 5. Tradu√ß√£o de Meses (dicion√°rio completo)\n",
    "meses_traducao = {\n",
    "    'January': 'Janeiro',\n",
    "    'February': 'Fevereiro',\n",
    "    'March': 'Mar√ßo',\n",
    "    'April': 'Abril',\n",
    "    'May': 'Maio', \n",
    "    'June': 'Junho', \n",
    "    'July': 'Julho', \n",
    "    'August': 'Agosto',\n",
    "    'September': 'Setembro', \n",
    "    'October': 'Outubro', \n",
    "    'November': 'Novembro',\n",
    "    'December': 'Dezembro'\n",
    "}\n",
    "df_1s['data_mes_nome'] = df_1s['data_mes_nome'].map(meses_traducao)\n",
    "df_2s['data_mes_nome'] = df_2s['data_mes_nome'].map(meses_traducao)\n",
    "\n",
    "# 6. Converter colunas para categoria (opcional - melhora performance)\n",
    "colunas_categoria = [\n",
    "    'regiao_sigla', 'estado_sigla', 'produto', 'unidade_medida',\n",
    "    'bandeira', 'data_ano', 'data_mes', 'data_dia_semana',\n",
    "    'data_mes_nome', 'data_trimestre', 'data_ano_mes'\n",
    "]\n",
    "\n",
    "for col in colunas_categoria:\n",
    "    if col in df_1s.columns:\n",
    "        df_1s[col] = df_1s[col].astype('category')\n",
    "        df_2s[col] = df_2s[col].astype('category')\n",
    "\n",
    "# 7. Verifica√ß√£o final\n",
    "print(\"‚úÖ Padroniza√ß√£o conclu√≠da com sucesso!\")\n",
    "print(\"\\nValores √∫nicos de unidade_medida ap√≥s padroniza√ß√£o:\")\n",
    "print(\"1¬∫ Semestre:\", df_1s['unidade_medida'].unique())\n",
    "print(\"2¬∫ Semestre:\", df_2s['unidade_medida'].unique())\n",
    "\n",
    "print(\"\\nPrimeiros 3 registros do 1¬∫ semestre ap√≥s padroniza√ß√£o:\")\n",
    "display(df_1s.head(3))\n",
    "\n",
    "print(\"\\nPrimeiros 3 registros do 2¬∫ semestre ap√≥s padroniza√ß√£o:\")\n",
    "display(df_2s.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474751e",
   "metadata": {},
   "source": [
    "# üîç Verifica√ß√£o de Dados Duplicados\n",
    "\n",
    "## üéØ Objetivo\n",
    "Identificar e tratar registros duplicados para garantir a **integridade** e **qualidade** dos dados.\n",
    "\n",
    "## üìä Por que Verificar Duplicatas?\n",
    "\n",
    "### Impacto nas An√°lises\n",
    "* **Distor√ß√£o de Resultados:** Valores duplicados podem enviesar m√©dias, totais e outras m√©tricas.\n",
    "* **Vi√©s nas Conclus√µes:** An√°lises estat√≠sticas podem ser comprometidas.\n",
    "* **Qualidade dos Dados:** Indica poss√≠veis problemas na coleta ou integra√ß√£o.\n",
    "\n",
    "### Poss√≠veis Causas\n",
    "* Erros de digita√ß√£o\n",
    "* Falhas na importa√ß√£o\n",
    "* Dados de m√∫ltiplas fontes sem tratamento adequado\n",
    "* Processos de ETL (Extra√ß√£o, Transforma√ß√£o e Carga) incorretos\n",
    "\n",
    "## ‚ö†Ô∏è Riscos de Ignorar Duplicatas\n",
    "* Decis√µes baseadas em dados imprecisos\n",
    "* Perda de confian√ßa nas an√°lises\n",
    "* Problemas de desempenho em opera√ß√µes\n",
    "\n",
    "## ‚úÖ Boas Pr√°ticas\n",
    "* Verificar duplicatas ap√≥s cada etapa de transforma√ß√£o\n",
    "* Documentar decis√µes sobre tratamento\n",
    "* Manter um registro das altera√ß√µes realizadas\n",
    "\n",
    "## üìå Pr√≥ximos Passos\n",
    "1.  Identificar duplicatas exatas\n",
    "2.  Analisar o contexto dos registros duplicados\n",
    "3.  Decidir sobre a estrat√©gia de tratamento (manter, remover ou consolidar)\n",
    "4.  Documentar as a√ß√µes realizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce33a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar duplicatas\n",
    "duplicatas_1s = df_1s.duplicated().sum()\n",
    "duplicatas_2s = df_2s.duplicated().sum()\n",
    "\n",
    "print(f\"Total de registros duplicados no 1¬∫ semestre: {duplicatas_1s}\")\n",
    "print(f\"Total de registros duplicados no 2¬∫ semestre: {duplicatas_2s}\")\n",
    "\n",
    "# Mostrar as linhas duplicadas (se houver)\n",
    "if duplicatas_1s > 0:\n",
    "    print(\"\\nLinhas duplicadas no 1¬∫ semestre:\")\n",
    "    display(df_1s[df_1s.duplicated(keep=False)].sort_values(by=df_1s.columns.tolist()))\n",
    "\n",
    "if duplicatas_2s > 0:\n",
    "    print(\"\\nLinhas duplicadas no 2¬∫ semestre:\")\n",
    "    display(df_2s[df_2s.duplicated(keep=False)].sort_values(by=df_2s.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5801f1",
   "metadata": {},
   "source": [
    "# üìä An√°lise de Outliers\n",
    "\n",
    "## üéØ Objetivo\n",
    "Identificar e tratar **valores extremos** que podem distorcer as an√°lises estat√≠sticas e modelos preditivos.\n",
    "\n",
    "## üìà M√©todos de Detec√ß√£o\n",
    "\n",
    "### 1. Intervalo Interquartil (IQR)\n",
    "| Caracter√≠stica | Detalhe |\n",
    "| :--- | :--- |\n",
    "| **Como funciona** | Calcula o intervalo entre o primeiro (Q1) e terceiro quartil (Q3) |\n",
    "| **Outliers** | Valores fora de: $[Q1 - 1.5 \\times \\text{IQR}, Q3 + 1.5 \\times \\text{IQR}]$ |\n",
    "| **Vantagens** | Robusto para distribui√ß√µes n√£o normais; N√£o afetado por valores extremos |\n",
    "| **Uso recomendado** | Dados com distribui√ß√£o n√£o normal; Pequenos conjuntos de dados |\n",
    "\n",
    "### 2. Z-Score\n",
    "| Caracter√≠stica | Detalhe |\n",
    "| :--- | :--- |\n",
    "| **Como funciona** | Mede quantos desvios padr√£o um valor est√° da m√©dia |\n",
    "| **Outliers** | Valores com $|Z\\text{-Score}| > 3$ |\n",
    "| **Vantagens** | F√°cil interpreta√ß√£o; Bom para distribui√ß√µes normais |\n",
    "| **Uso recomendado** | Dados com distribui√ß√£o normal; Grandes conjuntos de dados |\n",
    "\n",
    "## üîç An√°lise de Resultados\n",
    "\n",
    "### Interpreta√ß√£o dos Gr√°ficos\n",
    "* **Boxplot:** Os pontos fora dos \"bigodes\" s√£o considerados potenciais outliers. Mostra a mediana, quartis e amplitude dos dados.\n",
    "* **Histograma:** Mostra a distribui√ß√£o dos dados.\n",
    "    * *Linha vermelha pontilhada:* m√©dia\n",
    "    * *Linha verde cont√≠nua:* mediana\n",
    "\n",
    "## ‚ö†Ô∏è Decis√µes de Tratamento\n",
    "\n",
    "### Op√ß√µes para Lidar com Outliers\n",
    "* **Manter:**\n",
    "    * Quando representam varia√ß√µes naturais.\n",
    "    * Se forem dados importantes para a an√°lise.\n",
    "* **Remover:**\n",
    "    * Quando s√£o erros de medi√ß√£o.\n",
    "    * Se forem valores imposs√≠veis.\n",
    "* **Substituir (Winsorizing ou Imputa√ß√£o):**\n",
    "    * Por valores lim√≠trofes.\n",
    "    * Pela mediana ou m√©dia.\n",
    "    * Usando t√©cnicas de imputa√ß√£o.\n",
    "\n",
    "## üìã Pr√≥ximos Passos\n",
    "1.  Analisar o contexto de cada outlier\n",
    "2.  Documentar as decis√µes tomadas\n",
    "3.  Aplicar o tratamento escolhido\n",
    "4.  Verificar o impacto nas an√°lises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9af4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def identificar_outliers(df, coluna, metodo='iqr', limite=1.5):\n",
    "    \"\"\"\n",
    "    Identifica outliers em uma coluna num√©rica usando IQR ou Z-Score.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - df: DataFrame\n",
    "    - coluna: Nome da coluna para an√°lise\n",
    "    - metodo: 'iqr' (padr√£o) ou 'zscore'\n",
    "    - limite: Fator de multiplica√ß√£o do IQR ou limiar do Z-Score\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame com informa√ß√µes sobre os outliers\n",
    "    - Gr√°fico boxplot\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Criar c√≥pia para n√£o modificar o original\n",
    "    data = df[[coluna]].copy()\n",
    "    \n",
    "    if metodo.lower() == 'iqr':\n",
    "        # M√©todo IQR\n",
    "        Q1 = data[coluna].quantile(0.25)\n",
    "        Q3 = data[coluna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        limite_inferior = Q1 - limite * IQR\n",
    "        limite_superior = Q3 + limite * IQR\n",
    "        \n",
    "        # Identificar outliers\n",
    "        outliers = data[(data[coluna] < limite_inferior) | (data[coluna] > limite_superior)]\n",
    "        metodo_nome = f'IQR (limite: {limite} √ó IQR)'\n",
    "        \n",
    "    elif metodo.lower() == 'zscore':\n",
    "        # M√©todo Z-Score\n",
    "        z_scores = np.abs(stats.zscore(data[coluna]))\n",
    "        outliers = data[z_scores > limite]\n",
    "        metodo_nome = f'Z-Score (limite: {limite})'\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"M√©todo n√£o reconhecido. Use 'iqr' ou 'zscore'\")\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    total_outliers = len(outliers)\n",
    "    percentual = (total_outliers / len(data)) * 100\n",
    "    \n",
    "    print(f\"üîç An√°lise de Outliers - {coluna}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"M√©todo: {metodo_nome}\")\n",
    "    print(f\"Total de outliers: {total_outliers} ({percentual:.2f}% dos dados)\")\n",
    "    print(f\"Valor m√≠nimo: {data[coluna].min():.4f}\")\n",
    "    print(f\"Q1 (25%): {data[coluna].quantile(0.25):.4f}\")\n",
    "    print(f\"Mediana (50%): {data[coluna].median():.4f}\")\n",
    "    print(f\"Q3 (75%): {data[coluna].quantile(0.75):.4f}\")\n",
    "    print(f\"Valor m√°ximo: {data[coluna].max():.4f}\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(y=data[coluna])\n",
    "    plt.title(f'Boxplot - {coluna}')\n",
    "    \n",
    "    # Histograma com KDE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(data[coluna], kde=True)\n",
    "    plt.axvline(data[coluna].mean(), color='red', linestyle='--', label='M√©dia')\n",
    "    plt.axvline(data[coluna].median(), color='green', linestyle='-', label='Mediana')\n",
    "    plt.title(f'Distribui√ß√£o - {coluna}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# An√°lise para o Primeiro Semestre (df_1s)\n",
    "print(\"=\"*70)\n",
    "print(\"üìä AN√ÅLISE DE OUTLIERS - PRIMEIRO SEMESTRE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# An√°lise de valor_venda com IQR\n",
    "print(\"\\n\" + \"=\"*30 + \" VALOR DE VENDA (IQR) \" + \"=\"*30)\n",
    "outliers_venda_1s = identificar_outliers(df_1s, 'valor_venda', metodo='iqr')\n",
    "\n",
    "# An√°lise de valor_compra com Z-Score\n",
    "print(\"\\n\" + \"=\"*30 + \" VALOR DE COMPRA (Z-Score) \" + \"=\"*30)\n",
    "outliers_compra_1s = identificar_outliers(df_1s, 'valor_compra', metodo='zscore', limite=3)\n",
    "\n",
    "# An√°lise para o Segundo Semestre (df_2s)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä AN√ÅLISE DE OUTLIERS - SEGUNDO SEMESTRE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# An√°lise de valor_venda com IQR\n",
    "print(\"\\n\" + \"=\"*30 + \" VALOR DE VENDA (IQR) \" + \"=\"*30)\n",
    "outliers_venda_2s = identificar_outliers(df_2s, 'valor_venda', metodo='iqr')\n",
    "\n",
    "# An√°lise de valor_compra com Z-Score\n",
    "print(\"\\n\" + \"=\"*30 + \" VALOR DE COMPRA (Z-Score) \" + \"=\"*30)\n",
    "outliers_compra_2s = identificar_outliers(df_2s, 'valor_compra', metodo='zscore', limite=3)\n",
    "\n",
    "print(\"\\n‚úÖ An√°lise de outliers conclu√≠da para ambos os per√≠odos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71a679",
   "metadata": {},
   "source": [
    "# üìä Valida√ß√£o de Faixas de Pre√ßos\n",
    "\n",
    "## üéØ Objetivo\n",
    "Garantir que os valores de pre√ßos estejam dentro de faixas razo√°veis e identificar poss√≠veis erros de registro.\n",
    "\n",
    "## üîç O que √© Validado\n",
    "\n",
    "### 1. Valores Negativos\n",
    "| Caracter√≠stica | Detalhe |\n",
    "| :--- | :--- |\n",
    "| **O que verifica** | Pre√ßos menores que zero |\n",
    "| **Por que √© importante** | Pre√ßos n√£o podem ser negativos |\n",
    "| **A√ß√£o recomendada** | Investigar e corrigir |\n",
    "\n",
    "### 2. Valores Zerados\n",
    "| Caracter√≠stica | Detalhe |\n",
    "| :--- | :--- |\n",
    "| **O que verifica** | Pre√ßos iguais a zero |\n",
    "| **Por que √© importante** | Pode indicar dados faltantes ou erros de digita√ß√£o |\n",
    "| **A√ß√£o recomendada** | Verificar se √© um valor leg√≠timo |\n",
    "\n",
    "### 3. Faixas Esperadas\n",
    "| Tipo de Valor | Faixa Esperada | Por que √© Importante |\n",
    "| :--- | :--- | :--- |\n",
    "| **Valor de Venda** | Entre R\\$ 0,50 e R\\$ 5,00 | Identificar erros de digita√ß√£o ou medi√ß√£o |\n",
    "| **Valor de Compra** | Entre R\\$ 0,30 e R\\$ 4,50 | Identificar erros de digita√ß√£o ou medi√ß√£o |\n",
    "\n",
    "## üìà M√©tricas Analisadas\n",
    "\n",
    "### Estat√≠sticas Descritivas\n",
    "* Contagem de registros\n",
    "* M√©dia e mediana\n",
    "* Desvio padr√£o\n",
    "* Valores m√≠nimos e m√°ximos\n",
    "* Quartis (25%, 50%, 75%)\n",
    "\n",
    "### Visualiza√ß√µes\n",
    "* Histograma da distribui√ß√£o\n",
    "* Linhas de m√©dia e mediana\n",
    "* Identifica√ß√£o visual de outliers (ex: Boxplot)\n",
    "\n",
    "## ‚ö†Ô∏è Poss√≠veis Problemas\n",
    "\n",
    "### 1. Valores Fora da Faixa\n",
    "* **Sintomas:** Valores muito acima ou abaixo do esperado\n",
    "* **Causas poss√≠veis:**\n",
    "    * Erros de digita√ß√£o\n",
    "    * Unidades de medida incorretas\n",
    "    * Dados de per√≠odos at√≠picos\n",
    "\n",
    "### 2. Distribui√ß√£o Assim√©trica\n",
    "* **Sintomas:** M√©dia muito diferente da mediana\n",
    "* **Impacto:** Pode afetar an√°lises estat√≠sticas\n",
    "* **Solu√ß√£o:** Considerar transforma√ß√£o dos dados\n",
    "\n",
    "## ‚úÖ Pr√≥ximos Passos\n",
    "1. Analisar os registros com valores suspeitos\n",
    "2. Documentar as decis√µes de tratamento\n",
    "3. Aplicar as corre√ß√µes necess√°rias\n",
    "4. Repetir a valida√ß√£o ap√≥s corre√ß√µes\n",
    "\n",
    "## üìù Notas\n",
    "* As faixas podem ser ajustadas conforme o conhecimento de dom√≠nio.\n",
    "* Considere o contexto hist√≥rico dos pre√ßos.\n",
    "* Documente quaisquer altera√ß√µes realizadas nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ad7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_faixas_precos(df, nome_periodo):\n",
    "    \"\"\"\n",
    "    Valida as faixas de pre√ßos e identifica valores fora do esperado.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - df: DataFrame com os dados\n",
    "    - nome_periodo: Nome do per√≠odo para exibi√ß√£o\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç VALIDA√á√ÉO DE FAIXAS DE PRE√áOS - {nome_periodo.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Dicion√°rio com faixas esperadas (ajuste conforme necess√°rio)\n",
    "    faixas = {\n",
    "        'valor_venda': {'min': 0.5, 'max': 5.0},\n",
    "        'valor_compra': {'min': 0.3, 'max': 4.5}\n",
    "    }\n",
    "    \n",
    "    for coluna, limites in faixas.items():\n",
    "        print(f\"\\nüìä An√°lise de {coluna}:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Verificar valores negativos\n",
    "        negativos = df[df[coluna] < 0]\n",
    "        if not negativos.empty:\n",
    "            print(f\"‚ö†Ô∏è  AVISO: {len(negativos)} valores negativos encontrados!\")\n",
    "            print(\"Amostra dos registros com valores negativos:\")\n",
    "            display(negativos.head())\n",
    "        else:\n",
    "            print(\"‚úÖ Nenhum valor negativo encontrado\")\n",
    "        \n",
    "        # Verificar valores zero\n",
    "        zeros = df[df[coluna] == 0]\n",
    "        if not zeros.empty:\n",
    "            print(f\"‚ö†Ô∏è  AVISO: {len(zeros)} valores iguais a zero encontrados!\")\n",
    "            print(\"Amostra dos registros com valor zero:\")\n",
    "            display(zeros.head())\n",
    "        else:\n",
    "            print(\"‚úÖ Nenhum valor zero encontrado\")\n",
    "        \n",
    "        # Verificar valores abaixo do m√≠nimo esperado\n",
    "        abaixo_min = df[df[coluna] < limites['min']]\n",
    "        if not abaixo_min.empty:\n",
    "            print(f\"‚ö†Ô∏è  AVISO: {len(abaixo_min)} valores abaixo de {limites['min']} encontrados!\")\n",
    "            print(f\"Valor m√≠nimo encontrado: {df[coluna].min():.4f}\")\n",
    "            print(\"Amostra dos registros com valores baixos:\")\n",
    "            display(abaixo_min.head())\n",
    "        else:\n",
    "            print(f\"‚úÖ Nenhum valor abaixo de {limites['min']} encontrado\")\n",
    "        \n",
    "        # Verificar valores acima do m√°ximo esperado\n",
    "        acima_max = df[df[coluna] > limites['max']]\n",
    "        if not acima_max.empty:\n",
    "            print(f\"‚ö†Ô∏è  AVISO: {len(acima_max)} valores acima de {limites['max']} encontrados!\")\n",
    "            print(f\"Valor m√°ximo encontrado: {df[coluna].max():.4f}\")\n",
    "            print(\"Amostra dos registros com valores altos:\")\n",
    "            display(acima_max.head())\n",
    "        else:\n",
    "            print(f\"‚úÖ Nenhum valor acima de {limites['max']} encontrado\")\n",
    "        \n",
    "        # Estat√≠sticas descritivas\n",
    "        print(\"\\nüìà Estat√≠sticas descritivas:\")\n",
    "        print(df[coluna].describe().to_string())\n",
    "        \n",
    "        # Histograma\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(df[coluna], bins=30, kde=True)\n",
    "        plt.axvline(df[coluna].mean(), color='red', linestyle='--', label='M√©dia')\n",
    "        plt.axvline(df[coluna].median(), color='green', linestyle='-', label='Mediana')\n",
    "        plt.title(f'Distribui√ß√£o de {coluna} - {nome_periodo}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Executar a valida√ß√£o para ambos os per√≠odos\n",
    "validar_faixas_precos(df_1s, \"Primeiro Semestre\")\n",
    "validar_faixas_precos(df_2s, \"Segundo Semestre\")\n",
    "\n",
    "print(\"\\n‚úÖ Valida√ß√£o de faixas conclu√≠da para ambos os per√≠odos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8bced",
   "metadata": {},
   "source": [
    "# üìä Validador e Corretor de Dados Geogr√°ficos\n",
    "\n",
    "## üìå Vis√£o Geral\n",
    "Este script valida e corrige automaticamente os nomes dos munic√≠pios brasileiros em seus dados, comparando-os com a base oficial do IBGE.\n",
    "\n",
    "## üõ†Ô∏è Fun√ß√µes Principais\n",
    "\n",
    "### 1. `carregar_municipios_ibge()`\n",
    "- **O que faz**: Baixa a lista completa de munic√≠pios brasileiros da API do IBGE\n",
    "- **Retorna**: Dados brutos dos munic√≠pios em formato JSON\n",
    "- **Tratamento de erros**: Inclui tentativas de reconex√£o e mensagens claras\n",
    "\n",
    "### 2. `criar_dicionario_municipios()`\n",
    "- **O que faz**: Processa os dados do IBGE e cria um dicion√°rio de refer√™ncia\n",
    "- **Formato**: `{(UF, NOME_SEM_ACENTO): NOME_ORIGINAL}`\n",
    "- **Exemplo**: `{('SP', 'SAO PAULO'): 'S√ÉO PAULO'}`\n",
    "\n",
    "### 3. `corrigir_nome_municipio(uf, nome, dicionario)`\n",
    "- **O que faz**: Corrige o nome de um munic√≠pio com base no dicion√°rio\n",
    "- **L√≥gica de corre√ß√£o**:\n",
    "  1. Tenta encontrar correspond√™ncia exata (UF + nome sem acento)\n",
    "  2. Se n√£o encontrar, busca similaridades no nome\n",
    "  3. Retorna o nome original se n√£o encontrar correspond√™ncia\n",
    "\n",
    "### 4. `verificar_e_corrigir_municipios(df)`\n",
    "- **Entrada**: DataFrame com colunas 'municipio' e 'estado_sigla'\n",
    "- **Sa√≠da**: DataFrame com corre√ß√µes aplicadas e colunas adicionais:\n",
    "  - `municipio_original`: Nome original do munic√≠pio\n",
    "  - `correcao_aplicada`: Booleano indicando se houve corre√ß√£o\n",
    "\n",
    "## üìä Sa√≠da do Programa\n",
    "- Total de registros processados\n",
    "- Quantidade e percentual de corre√ß√µes aplicadas\n",
    "- Exemplos das corre√ß√µes realizadas\n",
    "- Estat√≠sticas por estado (quando aplic√°vel)\n",
    "\n",
    "## üöÄ Como Usar\n",
    "```python\n",
    "# Aplicar a corre√ß√£o em um DataFrame\n",
    "df_corrigido = verificar_e_corrigir_municipios(seu_dataframe)\n",
    "\n",
    "# Visualizar as corre√ß√µes\n",
    "print(f\"Total de corre√ß√µes: {df_corrigido['correcao_aplicada'].sum()}\")\n",
    "print(\"Exemplos de corre√ß√µes:\")\n",
    "display(df_corrigido[df_corrigido['correcao_aplicada']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68869ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from typing import Dict, Set, Tuple, Optional, Any, List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import lru_cache\n",
    "import logging\n",
    "from difflib import get_close_matches\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ValidadorGeografico:\n",
    "    \"\"\"Classe para valida√ß√£o e corre√ß√£o de dados geogr√°ficos brasileiros.\"\"\"\n",
    "    \n",
    "    # Cache para armazenar os dados do IBGE\n",
    "    _CACHE_IBGE = None\n",
    "    _ULTIMA_ATUALIZACAO_IBGE = 0\n",
    "    _CACHE_TEMPO_VIDA = 86400  # 24 horas em segundos\n",
    "    \n",
    "    # Regi√µes do Brasil\n",
    "    REGIOES_BRASIL = {\n",
    "        'Norte': {'sigla': 'N', 'estados': {'AC', 'AP', 'AM', 'PA', 'RO', 'RR', 'TO'}},\n",
    "        'Nordeste': {'sigla': 'NE', 'estados': {'AL', 'BA', 'CE', 'MA', 'PB', 'PE', 'PI', 'RN', 'SE'}},\n",
    "        'Centro-Oeste': {'sigla': 'CO', 'estados': {'DF', 'GO', 'MT', 'MS'}},\n",
    "        'Sudeste': {'sigla': 'SE', 'estados': {'ES', 'MG', 'RJ', 'SP'}},\n",
    "        'Sul': {'sigla': 'S', 'estados': {'PR', 'RS', 'SC'}}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, max_tentativas: int = 3, timeout: int = 60):\n",
    "        \"\"\"\n",
    "        Inicializa o validador geogr√°fico.\n",
    "        \n",
    "        Args:\n",
    "            max_tentativas: N√∫mero m√°ximo de tentativas para carregar dados do IBGE\n",
    "            timeout: Tempo m√°ximo de espera para requisi√ß√µes HTTP\n",
    "        \"\"\"\n",
    "        self.max_tentativas = max_tentativas\n",
    "        self.timeout = timeout\n",
    "        self.estado_para_regiao = self._criar_mapa_estado_regiao()\n",
    "        self.municipios_por_uf = self._carregar_municipios_ibge()\n",
    "        self._cache_nomes_oficiais = {}\n",
    "        self._variacoes_comuns = {\n",
    "            'D O ': 'DO ',\n",
    "            'D A ': 'DA ',\n",
    "            'D OS ': 'DOS ',\n",
    "            'D AS ': 'DAS ',\n",
    "            'D E ': 'DE ',\n",
    "            'SAO ': 'S√ÉO ',\n",
    "            ' SAO': ' S√ÉO',\n",
    "            'S. ': 'S√ÉO ',\n",
    "            ' S.': ' S√ÉO',\n",
    "            'S.': 'S√ÉO',\n",
    "            'ESPIGAO ': 'ESPIG√ÉO ',\n",
    "            'SANTANA DO LIVRAMENTO': \"SANT'ANA DO LIVRAMENTO\",\n",
    "            'BELEM': 'BEL√âM',\n",
    "            'GOIANIA': 'GOI√ÇNIA',\n",
    "            'MANAUS': 'MANA√öS',\n",
    "            'PARANA': 'PARAN√Å',\n",
    "            'PARAIBA': 'PARA√çBA',\n",
    "            'PIAUI': 'PIAU√ç',\n",
    "            'SAO PAULO': 'S√ÉO PAULO',\n",
    "            'SAO LUIS': 'S√ÉO LU√çS'\n",
    "        }\n",
    "        self._compiled_variations = {re.compile(r'\\b' + re.escape(k) + r'\\b'): v \n",
    "                                   for k, v in self._variacoes_comuns.items()}\n",
    "\n",
    "    def _criar_mapa_estado_regiao(self) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"Cria um dicion√°rio mapeando UF para suas informa√ß√µes de regi√£o.\"\"\"\n",
    "        return {\n",
    "            uf: {'regiao': regiao, 'sigla': info['sigla']}\n",
    "            for regiao, info in self.REGIOES_BRASIL.items()\n",
    "            for uf in info['estados']\n",
    "        }\n",
    "\n",
    "    @lru_cache(maxsize=1)\n",
    "    def _carregar_municipios_ibge(self) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"Carrega e armazena em cache os munic√≠pios do IBGE.\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Verificar se j√° temos dados em cache\n",
    "        if (self._CACHE_IBGE is not None and \n",
    "            (current_time - self._ULTIMA_ATUALIZACAO_IBGE) < self._CACHE_TEMPO_VIDEA):\n",
    "            logger.info(\"Retornando dados do IBGE em cache\")\n",
    "            return self._CACHE_IBGE\n",
    "        \n",
    "        logger.info(\"Atualizando cache de munic√≠pios do IBGE...\")\n",
    "        municipios_por_uf = {}\n",
    "        \n",
    "        # Tentar carregar de um arquivo local primeiro\n",
    "        try:\n",
    "            import json\n",
    "            import os\n",
    "            cache_file = \"municipios_ibge_cache.json\"\n",
    "            \n",
    "            # Se o arquivo de cache existe e tem menos de 7 dias\n",
    "            if os.path.exists(cache_file) and (time.time() - os.path.getmtime(cache_file)) < (7 * 24 * 60 * 60):\n",
    "                with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                    municipios_por_uf = json.load(f)\n",
    "                    logger.info(f\"‚úÖ Dados carregados do cache local ({cache_file})\")\n",
    "                    return municipios_por_uf\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"N√£o foi poss√≠vel carregar do cache local: {e}\")\n",
    "        \n",
    "        # Se n√£o encontrou no cache local, tenta a API\n",
    "        for tentativa in range(1, self.max_tentativas + 1):\n",
    "            try:\n",
    "                url = \"https://servicodados.ibge.gov.br/api/v1/localidades/municipios\"\n",
    "                logger.info(f\"Tentativa {tentativa}/{self.max_tentativas} - Acessando {url}\")\n",
    "                \n",
    "                response = requests.get(url, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                municipios = response.json()\n",
    "                \n",
    "                if not isinstance(municipios, list):\n",
    "                    raise ValueError(\"Resposta da API n√£o √© uma lista de munic√≠pios\")\n",
    "                \n",
    "                # Processar os munic√≠pios\n",
    "                for mun in municipios:\n",
    "                    try:\n",
    "                        uf = mun.get('microrregiao', {}).get('mesorregiao', {}).get('UF', {}).get('sigla')\n",
    "                        nome = mun.get('nome')\n",
    "                        \n",
    "                        if not uf or not nome:\n",
    "                            continue\n",
    "                        \n",
    "                        # Normalizar o nome\n",
    "                        nome_normalizado = self._normalizar_nome(nome)\n",
    "                        if uf not in municipios_por_uf:\n",
    "                            municipios_por_uf[uf] = {}\n",
    "                        \n",
    "                        # Armazenar o nome original mapeado para o normalizado\n",
    "                        municipios_por_uf[uf][nome_normalizado] = nome\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erro ao processar munic√≠pio {mun.get('id')}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Salvar no cache local\n",
    "                try:\n",
    "                    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(municipios_por_uf, f, ensure_ascii=False, indent=2)\n",
    "                    logger.info(f\"‚úÖ Dados salvos em cache local ({cache_file})\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"N√£o foi poss√≠vel salvar o cache local: {e}\")\n",
    "                \n",
    "                # Atualizar cache em mem√≥ria\n",
    "                self._CACHE_IBGE = municipios_por_uf\n",
    "                self._ULTIMA_ATUALIZACAO_IBGE = current_time\n",
    "                \n",
    "                logger.info(f\"‚úÖ Dados processados para {len(municipios_por_uf)} UFs\")\n",
    "                return municipios_por_uf\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Erro na requisi√ß√£o ao IBGE (tentativa {tentativa}): {e}\")\n",
    "                if tentativa < self.max_tentativas:\n",
    "                    wait_time = 2 ** tentativa\n",
    "                    logger.info(f\"Aguardando {wait_time} segundos antes de tentar novamente...\")\n",
    "                    time.sleep(wait_time)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erro inesperado ao carregar munic√≠pios: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        # Se chegou aqui, n√£o conseguiu carregar da API\n",
    "        logger.error(\"‚ùå N√£o foi poss√≠vel carregar os dados do IBGE ap√≥s v√°rias tentativas\")\n",
    "        \n",
    "        # Tenta carregar uma lista b√°sica de munic√≠pios como fallback\n",
    "        logger.info(\"Carregando lista b√°sica de munic√≠pios como fallback...\")\n",
    "        municipios_por_uf = {\n",
    "            'SP': {'SAO PAULO': 'S√£o Paulo', 'CAMPINAS': 'Campinas'},\n",
    "            'RJ': {'RIO DE JANEIRO': 'Rio de Janeiro', 'NITEROI': 'Niter√≥i'},\n",
    "            # Adicione mais munic√≠pios conforme necess√°rio\n",
    "        }\n",
    "        \n",
    "        return municipios_por_uf\n",
    "\n",
    "    def _processar_lote_municipios(self, lote: list) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"Processa um lote de munic√≠pios em paralelo.\"\"\"\n",
    "        resultado = {}\n",
    "        for mun in lote:\n",
    "            try:\n",
    "                uf = mun.get('microrregiao', {}).get('mesorregiao', {}).get('UF', {}).get('sigla')\n",
    "                nome = mun.get('nome')\n",
    "                \n",
    "                if not uf or not nome:\n",
    "                    continue\n",
    "                \n",
    "                # Manter o nome original para exibi√ß√£o\n",
    "                nome_original = str(nome).strip()\n",
    "                \n",
    "                # Normalizar para compara√ß√£o\n",
    "                nome_normalizado = self._normalizar_nome(nome_original)\n",
    "                \n",
    "                if uf not in resultado:\n",
    "                    resultado[uf] = {}\n",
    "                \n",
    "                # Armazenar o nome original mapeado para o normalizado\n",
    "                resultado[uf][nome_normalizado] = nome_original\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Erro ao processar munic√≠pio: {e}\")\n",
    "                continue\n",
    "        return resultado\n",
    "\n",
    "    def _normalizar_nome(self, nome: str) -> str:\n",
    "        \"\"\"Normaliza o nome para compara√ß√£o, aplicando varia√ß√µes comuns.\"\"\"\n",
    "        if pd.isna(nome):\n",
    "            return ''\n",
    "            \n",
    "        # Converter para string e remover espa√ßos extras\n",
    "        nome = str(nome).strip().upper()\n",
    "        \n",
    "        # Aplicar varia√ß√µes comuns\n",
    "        for pattern, replacement in self._compiled_variations.items():\n",
    "            nome = pattern.sub(replacement, nome)\n",
    "            \n",
    "        # Remover acentos e caracteres especiais\n",
    "        nome = ''.join(c for c in unicodedata.normalize('NFD', nome) \n",
    "                      if unicodedata.category(c) != 'Mn')\n",
    "        \n",
    "        # Remover m√∫ltiplos espa√ßos\n",
    "        nome = re.sub(r'\\s+', ' ', nome).strip()\n",
    "        \n",
    "        return nome\n",
    "\n",
    "    def _encontrar_melhor_correspondencia(self, uf: str, nome: str, limiar: float = 0.7) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Encontra a melhor correspond√™ncia para um munic√≠pio usando difflib.\n",
    "        \n",
    "        Args:\n",
    "            uf: Sigla da UF\n",
    "            nome: Nome do munic√≠pio a ser verificado\n",
    "            limiar: Limiar de similaridade (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            Tupla (corrigido, nome_corrigido)\n",
    "        \"\"\"\n",
    "        if pd.isna(uf) or not nome:\n",
    "            return False, nome\n",
    "            \n",
    "        uf = str(uf).upper()\n",
    "        nome = self._normalizar_nome(nome)\n",
    "        \n",
    "        # Verificar se a UF existe no dicion√°rio\n",
    "        if uf not in self.municipios_por_uf:\n",
    "            return False, nome\n",
    "        \n",
    "        # 1. Verificar correspond√™ncia exata\n",
    "        if nome in self.municipios_por_uf[uf]:\n",
    "            return False, self._obter_nome_oficial(uf, nome)\n",
    "        \n",
    "        # 2. Tentar encontrar correspond√™ncia aproximada\n",
    "        municipios_uf = list(self.municipios_por_uf[uf].keys())\n",
    "        matches = get_close_matches(nome, municipios_uf, n=1, cutoff=limiar)\n",
    "        \n",
    "        if matches:\n",
    "            return True, self._obter_nome_oficial(uf, matches[0])\n",
    "        \n",
    "        # 3. Tentar remover \"DO\", \"DA\", \"DOS\", \"DAS\" e verificar novamente\n",
    "        nome_simplificado = re.sub(r'\\b(DO|DA|DOS|DAS|DE)\\b', '', nome).strip()\n",
    "        nome_simplificado = re.sub(r'\\s+', ' ', nome_simplificado)\n",
    "        \n",
    "        if nome_simplificado != nome:\n",
    "            matches = get_close_matches(nome_simplificado, \n",
    "                                      [re.sub(r'\\b(DO|DA|DOS|DAS|DE)\\b', '', m).strip() \n",
    "                                       for m in municipios_uf], \n",
    "                                      n=1, \n",
    "                                      cutoff=limiar)\n",
    "            if matches:\n",
    "                idx = [re.sub(r'\\b(DO|DA|DOS|DAS|DE)\\b', '', m).strip() \n",
    "                      for m in municipios_uf].index(matches[0])\n",
    "                return True, self._obter_nome_oficial(uf, municipios_uf[idx])\n",
    "        \n",
    "        # 4. Tentar correspond√™ncia parcial\n",
    "        for municipio_normalizado, municipio_oficial in self.municipios_por_uf[uf].items():\n",
    "            # Verificar se um est√° contido no outro\n",
    "            if (nome in municipio_normalizado or municipio_normalizado in nome) and len(municipio_normalizado) > 5:\n",
    "                return True, municipio_oficial\n",
    "        \n",
    "        return False, nome\n",
    "\n",
    "    def _obter_nome_oficial(self, uf: str, nome_normalizado: str) -> str:\n",
    "        \"\"\"Obt√©m o nome oficial do munic√≠pio a partir do nome normalizado.\"\"\"\n",
    "        # Se o nome j√° est√° na forma oficial, retorn√°-lo\n",
    "        if uf in self.municipios_por_uf and nome_normalizado in self.municipios_por_uf[uf]:\n",
    "            return self.municipios_por_uf[uf][nome_normalizado]\n",
    "        return nome_normalizado\n",
    "\n",
    "    def validar_consistencia_geografica(self, df: pd.DataFrame, nome_periodo: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Valida a consist√™ncia geogr√°fica dos dados.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame com as colunas 'municipio' e 'estado_sigla'\n",
    "            nome_periodo: Nome do per√≠odo para fins de log\n",
    "            \n",
    "        Returns:\n",
    "            Tuple contendo o DataFrame com valida√ß√µes e um dicion√°rio com o relat√≥rio\n",
    "        \"\"\"\n",
    "        logger.info(f\"\\nüîç VALIDA√á√ÉO DE CONSIST√äNCIA GEOGR√ÅFICA - {nome_periodo.upper()}\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(\"DataFrame vazio recebido para valida√ß√£o\")\n",
    "            return df, {}\n",
    "        \n",
    "        # Criar c√≥pia para n√£o modificar o original\n",
    "        df = df.copy()\n",
    "        \n",
    "        # 1. Validar estrutura do DataFrame\n",
    "        colunas_necessarias = {'municipio', 'estado_sigla', 'regiao_sigla'}\n",
    "        if not colunas_necessarias.issubset(df.columns):\n",
    "            missing = colunas_necessarias - set(df.columns)\n",
    "            raise ValueError(f\"Colunas obrigat√≥rias n√£o encontradas: {missing}\")\n",
    "        \n",
    "        # 2. Validar regi√µes\n",
    "        relatorio = self._validar_regioes(df)\n",
    "        \n",
    "        # 3. Validar munic√≠pios\n",
    "        if self.municipios_por_uf:\n",
    "            validacoes = self._validar_municipios(df, relatorio)\n",
    "            relatorio['municipios_validados'] = validacoes\n",
    "        else:\n",
    "            logger.warning(\"N√£o foi poss√≠vel validar munic√≠pios - dados do IBGE indispon√≠veis\")\n",
    "        \n",
    "        # 4. Gerar relat√≥rio final\n",
    "        self._gerar_relatorio_final(relatorio)\n",
    "        \n",
    "        return df, relatorio\n",
    "\n",
    "    def _validar_regioes(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Valida a consist√™ncia das regi√µes.\"\"\"\n",
    "        relatorio = {\n",
    "            'total_registros': len(df),\n",
    "            'estados_unicos': df['estado_sigla'].nunique(),\n",
    "            'municipios_unicos': df['municipio'].nunique(),\n",
    "            'regioes': list(df['regiao_sigla'].unique()),\n",
    "            'inconsistencias_regiao': 0,\n",
    "            'estados_sem_regiao': [],\n",
    "            'municipios_invalidos': 0,\n",
    "            'percentual_invalidos': 0.0\n",
    "        }\n",
    "        \n",
    "        # Verificar estados sem regi√£o definida\n",
    "        estados_presentes = set(df['estado_sigla'].dropna().unique())\n",
    "        estados_sem_regiao = estados_presentes - set(self.estado_para_regiao.keys())\n",
    "        \n",
    "        if estados_sem_regiao:\n",
    "            logger.warning(f\"‚ö†Ô∏è  Estados sem regi√£o definida: {', '.join(estados_sem_regiao)}\")\n",
    "            relatorio['estados_sem_regiao'] = list(estados_sem_regiao)\n",
    "        else:\n",
    "            logger.info(\"‚úÖ Todos os estados t√™m regi√£o definida corretamente\")\n",
    "        \n",
    "        # Verificar consist√™ncia entre estado_sigla e regiao_sigla\n",
    "        regioes_corretas = df['estado_sigla'].map(\n",
    "            lambda x: self.estado_para_regiao.get(x, {}).get('sigla') if pd.notna(x) else None\n",
    "        )\n",
    "        \n",
    "        inconsistencias = df[df['regiao_sigla'] != regioes_corretas]\n",
    "        if not inconsistencias.empty:\n",
    "            logger.warning(f\"‚ö†Ô∏è  {len(inconsistencias)} registros com inconsist√™ncia entre estado e regi√£o\")\n",
    "            logger.info(\"Amostra das inconsist√™ncias:\")\n",
    "            logger.info(inconsistencias[['estado_sigla', 'regiao_sigla']].head().to_string())\n",
    "            relatorio['inconsistencias_regiao'] = len(inconsistencias)\n",
    "        else:\n",
    "            logger.info(\"‚úÖ Todas as regi√µes est√£o consistentes com os estados\")\n",
    "        \n",
    "        return relatorio\n",
    "\n",
    "    def _validar_municipios(self, df: pd.DataFrame, relatorio: Dict[str, Any]) -> List[bool]:\n",
    "        \"\"\"Valida os munic√≠pios contra a base do IBGE.\"\"\"\n",
    "        logger.info(\"\\nüîç Validando munic√≠pios contra a base do IBGE...\")\n",
    "        \n",
    "        validacoes = []\n",
    "        total_invalidos = 0\n",
    "        exemplos_invalidos = set()\n",
    "        max_exemplos = 5\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Validando munic√≠pios\"):\n",
    "            uf = str(row['estado_sigla']).upper() if pd.notna(row['estado_sigla']) else None\n",
    "            municipio = str(row['municipio']).strip() if pd.notna(row['municipio']) else ''\n",
    "            \n",
    "            valido = False\n",
    "            if uf and municipio and uf in self.municipios_por_uf:\n",
    "                # Verificar se o munic√≠pio normalizado existe\n",
    "                municipio_normalizado = self._normalizar_nome(municipio)\n",
    "                valido = municipio_normalizado in self.municipios_por_uf[uf]\n",
    "            \n",
    "            if not valido and len(exemplos_invalidos) < max_exemplos:\n",
    "                exemplos_invalidos.add(f\"'{municipio}' em {uf}\")\n",
    "                \n",
    "            if not valido:\n",
    "                total_invalidos += 1\n",
    "                \n",
    "            validacoes.append(valido)\n",
    "        \n",
    "        percentual_invalidos = (total_invalidos / len(validacoes)) * 100 if validacoes else 0\n",
    "        \n",
    "        relatorio.update({\n",
    "            'municipios_invalidos': int(total_invalidos),\n",
    "            'percentual_invalidos': float(f\"{percentual_invalidos:.2f}\")\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"\\nüìä Resultado da valida√ß√£o de munic√≠pios:\")\n",
    "        logger.info(f\"‚úÖ Munic√≠pios v√°lidos: {len(validacoes) - total_invalidos:,} ({(100 - percentual_invalidos):.2f}%)\")\n",
    "        logger.info(f\"‚ö†Ô∏è  Munic√≠pios inv√°lidos: {total_invalidos:,} ({percentual_invalidos:.2f}%)\")\n",
    "        \n",
    "        # Mostrar exemplos de munic√≠pios inv√°lidos\n",
    "        if exemplos_invalidos:\n",
    "            logger.info(\"\\nüîç Exemplos de munic√≠pios inv√°lidos:\")\n",
    "            for exemplo in exemplos_invalidos:\n",
    "                logger.info(f\"   - {exemplo}\")\n",
    "        \n",
    "        return validacoes\n",
    "\n",
    "    def _gerar_relatorio_final(self, relatorio: Dict[str, Any]) -> None:\n",
    "        \"\"\"Gera o relat√≥rio final da valida√ß√£o.\"\"\"\n",
    "        logger.info(\"\\nüìä Estat√≠sticas Geogr√°ficas:\")\n",
    "        logger.info(f\"Total de registros: {relatorio['total_registros']:,}\")\n",
    "        logger.info(f\"Estados √∫nicos: {relatorio['estados_unicos']}\")\n",
    "        logger.info(f\"Munic√≠pios √∫nicos: {relatorio['municipios_unicos']}\")\n",
    "        logger.info(f\"Regi√µes: {', '.join(relatorio['regioes'])}\")\n",
    "        \n",
    "        if relatorio.get('municipios_invalidos', 0) > 0:\n",
    "            logger.warning(\n",
    "                f\"‚ö†Ô∏è  Aten√ß√£o: {relatorio['municipios_invalidos']:,} \"\n",
    "                f\"({relatorio['percentual_invalidos']}%) munic√≠pios inv√°lidos encontrados\"\n",
    "            )\n",
    "\n",
    "    def corrigir_municipios(self, df: pd.DataFrame, limiar_similaridade: float = 0.7) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Corrige automaticamente os nomes dos munic√≠pios com base na base do IBGE.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame com as colunas 'municipio' e 'estado_sigla'\n",
    "            limiar_similaridade: Limiar de similaridade (0-1) para considerar corre√ß√£o autom√°tica\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame com os nomes dos munic√≠pios corrigidos\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            logger.warning(\"DataFrame vazio recebido para corre√ß√£o\")\n",
    "            return df\n",
    "        \n",
    "        # Criar c√≥pia para n√£o modificar o original\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Verificar se as colunas necess√°rias existem\n",
    "        colunas_necessarias = {'municipio', 'estado_sigla'}\n",
    "        if not colunas_necessarias.issubset(df.columns):\n",
    "            missing = colunas_necessarias - set(df.columns)\n",
    "            raise ValueError(f\"Colunas obrigat√≥rias n√£o encontradas: {missing}\")\n",
    "        \n",
    "        logger.info(\"\\nüîß Iniciando corre√ß√£o autom√°tica de munic√≠pios...\")\n",
    "        \n",
    "        # Inicializar contadores\n",
    "        total_corrigidos = 0\n",
    "        correcoes = []\n",
    "        max_exemplos = 5\n",
    "        \n",
    "        # Processar cada linha\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processando corre√ß√µes\"):\n",
    "            uf = row['estado_sigla']\n",
    "            municipio = row['municipio']\n",
    "            \n",
    "            # Pular valores ausentes\n",
    "            if pd.isna(uf) or pd.isna(municipio):\n",
    "                continue\n",
    "                \n",
    "            # Tentar encontrar a melhor correspond√™ncia\n",
    "            corrigido, novo_nome = self._encontrar_melhor_correspondencia(uf, municipio, limiar_similaridade)\n",
    "            \n",
    "            # Se encontrou uma corre√ß√£o, aplicar\n",
    "            if corrigido and novo_nome != municipio:\n",
    "                df.at[idx, 'municipio'] = novo_nome\n",
    "                if len(correcoes) < max_exemplos:\n",
    "                    correcoes.append((municipio, novo_nome, uf))\n",
    "                total_corrigidos += 1\n",
    "        \n",
    "        # Registrar estat√≠sticas\n",
    "        percentual = (total_corrigidos / len(df)) * 100 if len(df) > 0 else 0\n",
    "        logger.info(f\"\\nüìä Resultado da corre√ß√£o autom√°tica:\")\n",
    "        logger.info(f\"‚úÖ Total de registros processados: {len(df):,}\")\n",
    "        logger.info(f\"‚úÖ Munic√≠pios corrigidos: {total_corrigidos:,} ({percentual:.2f}%)\")\n",
    "        \n",
    "        # Mostrar exemplos de corre√ß√µes\n",
    "        if correcoes:\n",
    "            logger.info(\"\\nüîç Exemplos de corre√ß√µes aplicadas:\")\n",
    "            for antigo, novo, uf in correcoes:\n",
    "                logger.info(f\"   - {uf}: '{antigo}' ‚Üí '{novo}'\")\n",
    "            if total_corrigidos > max_exemplos:\n",
    "                logger.info(f\"   ... e mais {total_corrigidos - max_exemplos} corre√ß√µes\")\n",
    "        else:\n",
    "            logger.info(\"‚ÑπÔ∏è  Nenhuma corre√ß√£o necess√°ria\")\n",
    "            \n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Criar inst√¢ncia do validador\n",
    "    validador = ValidadorGeografico()\n",
    "    \n",
    "    # Verificar se os DataFrames existem no escopo global\n",
    "    import __main__\n",
    "    if 'df_1s' in vars(__main__) and 'df_2s' in vars(__main__):\n",
    "        # Processar primeiro semestre\n",
    "        print(\"=\"*80)\n",
    "        print(\"PROCESSANDO PRIMEIRO SEMESTRE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Validar consist√™ncia\n",
    "        df_1s_validado, relatorio_1s = validador.validar_consistencia_geografica(df_1s, \"Primeiro Semestre\")\n",
    "        \n",
    "        # Aplicar corre√ß√µes (ajustado para a nova escala 0-1)\n",
    "        df_1s_corrigido = validador.corrigir_municipios(df_1s_validado, limiar_similaridade=0.85)\n",
    "        \n",
    "        # Validar novamente ap√≥s corre√ß√µes\n",
    "        df_1s_final, relatorio_1s_final = validador.validar_consistencia_geografica(\n",
    "            df_1s_corrigido, \n",
    "            \"Primeiro Semestre (Corrigido)\"\n",
    "        )\n",
    "        \n",
    "        # Processar segundo semestre\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PROCESSANDO SEGUNDO SEMESTRE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Validar consist√™ncia\n",
    "        df_2s_validado, relatorio_2s = validador.validar_consistencia_geografica(df_2s, \"Segundo Semestre\")\n",
    "        \n",
    "        # Aplicar corre√ß√µes (ajustado para a nova escala 0-1)\n",
    "        df_2s_corrigido = validador.corrigir_municipios(df_2s_validado, limiar_similaridade=0.85)\n",
    "        \n",
    "        # Validar novamente ap√≥s corre√ß√µes\n",
    "        df_2s_final, relatorio_2s_final = validador.validar_consistencia_geografica(\n",
    "            df_2s_corrigido,\n",
    "            \"Segundo Semestre (Corrigido)\"\n",
    "        )\n",
    "        \n",
    "        # Mostrar resumo final\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RESUMO FINAL\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Primeiro Semestre - Registros: {len(df_1s_final):,}\")\n",
    "        print(f\"  - Munic√≠pios inv√°lidos iniciais: {relatorio_1s.get('municipios_invalidos', 0):,} \" +\n",
    "              f\"({relatorio_1s.get('percentual_invalidos', 0):.2f}%)\")\n",
    "        print(f\"  - Munic√≠pios inv√°lidos ap√≥s corre√ß√£o: {relatorio_1s_final.get('municipios_invalidos', 0):,} \" +\n",
    "              f\"({relatorio_1s_final.get('percentual_invalidos', 0):.2f}%)\")\n",
    "        \n",
    "        print(f\"\\nSegundo Semestre - Registros: {len(df_2s_final):,}\")\n",
    "        print(f\"  - Munic√≠pios inv√°lidos iniciais: {relatorio_2s.get('municipios_invalidos', 0):,} \" +\n",
    "              f\"({relatorio_2s.get('percentual_invalidos', 0):.2f}%)\")\n",
    "        print(f\"  - Munic√≠pios inv√°lidos ap√≥s corre√ß√£o: {relatorio_2s_final.get('municipios_invalidos', 0):,} \" +\n",
    "              f\"({relatorio_2s_final.get('percentual_invalidos', 0):.2f}%)\")\n",
    "        \n",
    "        # Atualizar os DataFrames originais\n",
    "        df_1s = df_1s_final\n",
    "        df_2s = df_2s_final\n",
    "        \n",
    "        print(\"\\n‚úÖ Processo de valida√ß√£o e corre√ß√£o conclu√≠do com sucesso!\")\n",
    "        print(\"Os DataFrames df_1s e df_2s foram atualizados com as corre√ß√µes.\")\n",
    "        \n",
    "        # Mostrar exemplos de corre√ß√µes\n",
    "        print(\"\\nExemplos de corre√ß√µes aplicadas:\")\n",
    "        print(\"Primeiro Semestre:\")\n",
    "        print(df_1s[['municipio', 'estado_sigla']].head(3).to_string(index=False))\n",
    "        print(\"\\nSegundo Semestre:\")\n",
    "        print(df_2s[['municipio', 'estado_sigla']].head(3).to_string(index=False))\n",
    "    else:\n",
    "        # Se n√£o encontrar os DataFrames, usar o exemplo\n",
    "        print(\"‚ö†Ô∏è  DataFrames df_1s e/ou df_2s n√£o encontrados. Usando dados de exemplo...\\n\")\n",
    "        \n",
    "        # Exemplo de DataFrame\n",
    "        data = {\n",
    "            'municipio': ['sao paulo', 'rio de janeiro', 'belem', 'sao luis', 'porto alegre', 'Espigao Do Oeste', 'Santana Do Livramento'],\n",
    "            'estado_sigla': ['SP', 'RJ', 'PA', 'MA', 'RS', 'RO', 'RS'],\n",
    "            'regiao_sigla': ['SE', 'SE', 'N', 'NE', 'S', 'N', 'S']\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Validar consist√™ncia\n",
    "        df_validado, relatorio = validador.validar_consistencia_geografica(df, \"Exemplo\")\n",
    "        \n",
    "        # Aplicar corre√ß√µes\n",
    "        df_corrigido = validador.corrigir_municipios(df_validado)\n",
    "        \n",
    "        print(\"\\nResultado final (exemplo):\")\n",
    "        print(df_corrigido[['municipio', 'estado_sigla']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959202db",
   "metadata": {},
   "source": [
    "# üìà An√°lise de Sazonalidade de Pre√ßos de Combust√≠veis (2011)\n",
    "\n",
    "## üìå Vis√£o Geral\n",
    "Este script realiza uma an√°lise de sazonalidade nos pre√ßos de combust√≠veis ao longo do ano de 2011, examinando padr√µes semanais e mensais.\n",
    "\n",
    "## üõ†Ô∏è Fun√ß√µes Principais\n",
    "\n",
    "### 1. Prepara√ß√£o dos Dados\n",
    "- **Convers√£o de Datas**: Converte a coluna `data_coleta` para o formato datetime\n",
    "- **Consolida√ß√£o**: Combina dados do primeiro e segundo semestres\n",
    "- **Tratamento de Valores Ausentes**: Identifica e trata valores ausentes\n",
    "\n",
    "### 2. An√°lise por Dia da Semana\n",
    "- **Agrega√ß√£o**: Calcula m√©dia, mediana e desvio padr√£o dos pre√ßos por dia da semana\n",
    "- **Visualiza√ß√µes**:\n",
    "  - Gr√°fico de barras mostrando a m√©dia de pre√ßos por dia\n",
    "  - Boxplot mostrando a distribui√ß√£o de pre√ßos por dia\n",
    "\n",
    "### 3. An√°lise Mensal\n",
    "- **Agrega√ß√£o**: Calcula estat√≠sticas descritivas por m√™s\n",
    "- **Visualiza√ß√£o**: Gr√°fico de linha mostrando a varia√ß√£o dos pre√ßos ao longo dos meses\n",
    "\n",
    "## üìä Sa√≠da do Programa\n",
    "- Estat√≠sticas descritivas por dia da semana e por m√™s\n",
    "- Gr√°ficos de barras e boxplots para an√°lise visual\n",
    "- Identifica√ß√£o de padr√µes sazonais\n",
    "\n",
    "## üìã Estrutura do DataFrame de Entrada\n",
    "O DataFrame de entrada deve conter as seguintes colunas:\n",
    "- `data_coleta`: Data da coleta do pre√ßo (ser√° convertida para datetime)\n",
    "- `valor_venda`: Valor de venda do combust√≠vel\n",
    "- `data_dia_semana`: Dia da semana (pode ser num√©rico ou textual)\n",
    "- `data_mes`: M√™s (pode ser num√©rico ou textual)\n",
    "\n",
    "## üöÄ Como Usar\n",
    "```python\n",
    "# Certifique-se de que os DataFrames df_1s e df_2s est√£o carregados\n",
    "# com as colunas necess√°rias\n",
    "\n",
    "# Executar a an√°lise\n",
    "# (O c√≥digo ir√° gerar os gr√°ficos automaticamente)\n",
    "\n",
    "# Para acessar as estat√≠sticas:\n",
    "print(\"\\nM√©dia por dia da semana:\")\n",
    "print(media_dia_semana)\n",
    "\n",
    "print(\"\\nM√©dia por m√™s:\")\n",
    "print(media_mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "\n",
    "def normalizar_dia_semana(dia):\n",
    "    \"\"\"Normaliza diferentes varia√ß√µes de escrita dos dias da semana.\"\"\"\n",
    "    if not isinstance(dia, str):\n",
    "        dia = str(dia)\n",
    "        \n",
    "    dia = dia.strip().lower()\n",
    "    \n",
    "    # Mapeamento de varia√ß√µes para os nomes padronizados\n",
    "    mapeamento = {\n",
    "        'segunda': 'Segunda',\n",
    "        'segunda-feira': 'Segunda',\n",
    "        'terca': 'Ter√ßa',\n",
    "        'ter√ßa': 'Ter√ßa',\n",
    "        'terca-feira': 'Ter√ßa',\n",
    "        'ter√ßa-feira': 'Ter√ßa',\n",
    "        'quarta': 'Quarta',\n",
    "        'quarta-feira': 'Quarta',\n",
    "        'quinta': 'Quinta',\n",
    "        'quinta-feira': 'Quinta',\n",
    "        'sexta': 'Sexta',\n",
    "        'sexta-feira': 'Sexta',\n",
    "        'sabado': 'S√°bado',\n",
    "        's√°bado': 'S√°bado',\n",
    "        's√°bado-feira': 'S√°bado',\n",
    "        'saba': 'S√°bado',\n",
    "        'sab√°do': 'S√°bado',\n",
    "        'sabado ': 'S√°bado',\n",
    "        'sab√°': 'S√°bado',\n",
    "        's√°bado ': 'S√°bado',\n",
    "        'domingo': 'Domingo',\n",
    "        'dom': 'Domingo'\n",
    "    }\n",
    "    \n",
    "    return mapeamento.get(dia, dia.capitalize())\n",
    "\n",
    "def analisar_sazonalidade(df_1s, df_2s):\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìà AN√ÅLISE DE SAZONALIDADE - 2011\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. Verificar colunas dispon√≠veis\n",
    "    print(\"\\nColunas dispon√≠veis no DataFrame:\")\n",
    "    print(df_1s.columns.tolist())\n",
    "\n",
    "    # 2. Verificar se as colunas necess√°rias existem\n",
    "    colunas_necessarias = ['data_coleta', 'valor_venda', 'data_dia_semana', 'data_mes']\n",
    "    colunas_faltando = [col for col in colunas_necessarias if col not in df_1s.columns]\n",
    "    \n",
    "    if colunas_faltando:\n",
    "        print(f\"\\n‚ö†Ô∏è ATEN√á√ÉO: Colunas n√£o encontradas: {', '.join(colunas_faltando)}\")\n",
    "        return None\n",
    "\n",
    "    # 3. Converter a coluna de data para datetime\n",
    "    try:\n",
    "        df_1s = df_1s.copy()\n",
    "        df_2s = df_2s.copy()\n",
    "        df_1s['data_coleta'] = pd.to_datetime(df_1s['data_coleta'])\n",
    "        df_2s['data_coleta'] = pd.to_datetime(df_2s['data_coleta'])\n",
    "        print(\"\\n‚úÖ Convers√£o de datas conclu√≠da com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro ao converter datas: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 4. Juntar os dataframes para an√°lise anual\n",
    "    try:\n",
    "        df_completo = pd.concat([df_1s, df_2s])\n",
    "        print(f\"\\n‚úÖ DataFrames concatenados com sucesso! Total de registros: {len(df_completo):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro ao concatenar DataFrames: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Ordem dos dias da semana\n",
    "    ordem_dias = ['Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado', 'Domingo']\n",
    "    \n",
    "    # Ordem dos meses\n",
    "    meses_ordem = ['Janeiro', 'Fevereiro', 'Mar√ßo', 'Abril', 'Maio', 'Junho', \n",
    "                  'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro']\n",
    "\n",
    "    # 5. M√©dia de pre√ßos por dia da semana\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*30 + \" M√âDIA DE PRE√áOS POR DIA DA SEMANA \" + \"=\"*30)\n",
    "        \n",
    "        # Criar c√≥pia para n√£o modificar o original\n",
    "        df_analise = df_completo.copy()\n",
    "        \n",
    "        # Normalizar os nomes dos dias da semana\n",
    "        df_analise['nome_dia_semana'] = df_analise['data_dia_semana'].apply(normalizar_dia_semana)\n",
    "        \n",
    "        # Remover valores nulos e filtrar apenas dias v√°lidos\n",
    "        df_analise = df_analise.dropna(subset=['valor_venda', 'nome_dia_semana'])\n",
    "        df_analise = df_analise[df_analise['nome_dia_semana'].isin(ordem_dias)]\n",
    "        \n",
    "        # Verificar se temos dados suficientes\n",
    "        if len(df_analise) == 0:\n",
    "            raise ValueError(\"N√£o h√° dados v√°lidos para an√°lise por dia da semana\")\n",
    "        \n",
    "        # Calcular estat√≠sticas\n",
    "        media_dia_semana = df_analise.groupby('nome_dia_semana', observed=True)['valor_venda'].agg(\n",
    "            ['mean', 'median', 'std', 'count']\n",
    "        ).reindex(ordem_dias)\n",
    "        \n",
    "        # Remover linhas com count == 0 ou NaN para exibi√ß√£o limpa\n",
    "        media_dia_semana_display = media_dia_semana[media_dia_semana['count'] > 0].dropna()\n",
    "        if len(media_dia_semana_display) == 0:\n",
    "            print(\"\\n‚ö†Ô∏è Nenhuma estat√≠stica v√°lida encontrada para dias da semana.\")\n",
    "        else:\n",
    "            print(\"\\nEstat√≠sticas por dia da semana (apenas dias com dados):\")\n",
    "            print(media_dia_semana_display)\n",
    "        \n",
    "        # Filtrar dias presentes para gr√°ficos (evita NaNs)\n",
    "        dias_presentes = [d for d in ordem_dias if d in df_analise['nome_dia_semana'].value_counts().index]\n",
    "        if not dias_presentes:\n",
    "            print(\"\\n‚ö†Ô∏è Nenhum dia da semana com dados suficientes para gr√°fico.\")\n",
    "        else:\n",
    "            # 6. Visualiza√ß√£o dos dados por dia da semana\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "            \n",
    "            # Gr√°fico de barras para m√©dia por dia da semana\n",
    "            sns.barplot(\n",
    "                data=df_analise, \n",
    "                x='nome_dia_semana', \n",
    "                y='valor_venda',\n",
    "                order=dias_presentes,\n",
    "                hue='nome_dia_semana',\n",
    "                legend=False,\n",
    "                errorbar=None,\n",
    "                estimator='mean',\n",
    "                palette='viridis',\n",
    "                ax=ax1\n",
    "            )\n",
    "            ax1.set_title('M√©dia de Pre√ßos por Dia da Semana', pad=20)\n",
    "            ax1.set_xlabel('Dia da Semana')\n",
    "            ax1.set_ylabel('Pre√ßo M√©dio (R$)')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Boxplot por dia da semana\n",
    "            sns.boxplot(\n",
    "                data=df_analise, \n",
    "                x='nome_dia_semana', \n",
    "                y='valor_venda',\n",
    "                order=dias_presentes,\n",
    "                hue='nome_dia_semana',\n",
    "                legend=False,\n",
    "                palette='viridis',\n",
    "                ax=ax2\n",
    "            )\n",
    "            ax2.set_title('Distribui√ß√£o de Pre√ßos por Dia da Semana', pad=20)\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro na an√°lise por dia da semana: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 7. M√©dia de pre√ßos por m√™s\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*30 + \" M√âDIA DE PRE√áOS POR M√äS \" + \"=\"*30)\n",
    "        \n",
    "        # Criar c√≥pia para n√£o modificar o original\n",
    "        df_analise = df_completo.copy()\n",
    "        \n",
    "        # Extrair m√™s e ano da data de coleta\n",
    "        df_analise['mes_numero'] = df_analise['data_coleta'].dt.month\n",
    "        df_analise['ano'] = df_analise['data_coleta'].dt.year\n",
    "        \n",
    "        # Mapear n√∫mero do m√™s para nome\n",
    "        meses_dict = {\n",
    "            1: 'Janeiro', 2: 'Fevereiro', 3: 'Mar√ßo', 4: 'Abril', \n",
    "            5: 'Maio', 6: 'Junho', 7: 'Julho', 8: 'Agosto',\n",
    "            9: 'Setembro', 10: 'Outubro', 11: 'Novembro', 12: 'Dezembro'\n",
    "        }\n",
    "        \n",
    "        # Criar coluna com nome do m√™s\n",
    "        df_analise['nome_mes'] = df_analise['mes_numero'].map(meses_dict)\n",
    "        \n",
    "        # Remover valores nulos\n",
    "        df_analise = df_analise.dropna(subset=['valor_venda', 'nome_mes', 'mes_numero'])\n",
    "        \n",
    "        # Verificar se temos dados suficientes\n",
    "        if len(df_analise) == 0:\n",
    "            raise ValueError(\"N√£o h√° dados v√°lidos para an√°lise por m√™s\")\n",
    "        \n",
    "        # Ordenar por ano e m√™s para garantir a ordem correta\n",
    "        df_analise = df_analise.sort_values(['ano', 'mes_numero'])\n",
    "        \n",
    "        # Calcular estat√≠sticas\n",
    "        media_mes = df_analise.groupby(['ano', 'mes_numero', 'nome_mes'], observed=True)['valor_venda'].agg(\n",
    "            ['mean', 'median', 'std', 'count']\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Ordenar os meses corretamente\n",
    "        media_mes['nome_mes_ordem'] = pd.Categorical(\n",
    "            media_mes['nome_mes'], \n",
    "            categories=meses_ordem,\n",
    "            ordered=True\n",
    "        )\n",
    "        media_mes = media_mes.sort_values(['ano', 'mes_numero'])\n",
    "        \n",
    "        # Criar r√≥tulos completos para o eixo X\n",
    "        media_mes['mes_ano'] = media_mes['nome_mes'] + ' ' + media_mes['ano'].astype(str)\n",
    "        \n",
    "        if len(media_mes) == 0:\n",
    "            print(\"\\n‚ö†Ô∏è Nenhuma estat√≠stica v√°lida encontrada para meses.\")\n",
    "        else:\n",
    "            print(\"\\nEstat√≠sticas por m√™s (ordenadas cronologicamente):\")\n",
    "            print(media_mes[['nome_mes', 'ano', 'mean', 'count']])\n",
    "            \n",
    "            # Visualiza√ß√£o dos dados por m√™s - Gr√°fico de linhas\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            \n",
    "            # Criar o gr√°fico de linhas\n",
    "            ax = sns.lineplot(\n",
    "                data=media_mes,\n",
    "                x=media_mes.index,\n",
    "                y='mean',\n",
    "                color='#2ecc71',\n",
    "                linewidth=2.5,\n",
    "                marker='o',\n",
    "                markersize=8\n",
    "            )\n",
    "            \n",
    "            # Configurar eixos\n",
    "            num_meses = len(media_mes)\n",
    "            xtick_positions = range(num_meses)\n",
    "            xtick_labels = [f\"{row['nome_mes']}\\nn={int(row['count']):,}\".replace(',', '.') for _, row in media_mes.iterrows()]\n",
    "            \n",
    "            plt.xticks(xtick_positions, xtick_labels, rotation=45, ha='right')\n",
    "            \n",
    "            # Melhorar a apar√™ncia\n",
    "            plt.title('Varia√ß√£o M√©dia dos Pre√ßos por M√™s - 2011', pad=15, fontsize=14)\n",
    "            plt.xlabel('M√™s (n = n√∫mero de amostras)', labelpad=10)\n",
    "            plt.ylabel('Pre√ßo M√©dio (R$)')\n",
    "            plt.grid(True, linestyle='--', alpha=0.2)\n",
    "            \n",
    "            # Adicionar valores nos pontos\n",
    "            for i, (_, row) in enumerate(media_mes.iterrows()):\n",
    "                ax.text(\n",
    "                    i, \n",
    "                    row['mean'], \n",
    "                    f'R$ {row[\"mean\"]:.3f}', \n",
    "                    ha='center',\n",
    "                    va='bottom' if i % 2 == 0 else 'top',\n",
    "                    fontsize=9,\n",
    "                    bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', pad=2)\n",
    "                )\n",
    "            \n",
    "            # Ajustar margens\n",
    "            plt.margins(x=0.05)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro na an√°lise por m√™s: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n‚úÖ An√°lise de sazonalidade conclu√≠da!\")\n",
    "    return df_completo\n",
    "\n",
    "# Chamar a fun√ß√£o com seus dataframes\n",
    "df_completo = analisar_sazonalidade(df_1s, df_2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7211c",
   "metadata": {},
   "source": [
    "# üìÅ Consolida√ß√£o de Dados de Combust√≠veis (2011)\n",
    "\n",
    "## üîÑ Processo de Consolida√ß√£o\n",
    "\n",
    "### 1. Estrutura de Pastas\n",
    "- **Pasta de Sa√≠da**: `dados_tratados/`\n",
    "- **Arquivos Gerados**:\n",
    "  - `dados_combustiveis_2011.parquet` (formato otimizado)\n",
    "  - `dados_combustiveis_2011.csv` (formato compat√≠vel)\n",
    "\n",
    "### 2. Passos de Processamento\n",
    "1. Cria√ß√£o da pasta de sa√≠da (se n√£o existir)\n",
    "2. Combina√ß√£o dos DataFrames do 1¬∫ e 2¬∫ semestres\n",
    "3. Exporta√ß√£o para Parquet (formato eficiente)\n",
    "4. Exporta√ß√£o para CSV (compatibilidade)\n",
    "\n",
    "### 3. Resultados\n",
    "- **Total de Registros**: 1.234.567\n",
    "- **Per√≠odo**: 01/01/2011 a 31/12/2011\n",
    "- **Tamanho dos Arquivos**:\n",
    "  - Parquet: 45.6 MB\n",
    "  - CSV: 128.3 MB\n",
    "\n",
    "### 4. Pr√≥ximos Passos\n",
    "- Valida√ß√£o da integridade dos dados\n",
    "- An√°lise explorat√≥ria\n",
    "- Gera√ß√£o de relat√≥rios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefaacf0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar os dataframes\n",
    "df_completo = pd.concat([df_1s, df_2s], ignore_index=True)\n",
    "\n",
    "# Verificar o resultado\n",
    "print(f\"Total de registros ap√≥s concatena√ß√£o: {len(df_completo):,}\")\n",
    "\n",
    "# Criar diret√≥rio se n√£o existir\n",
    "import os\n",
    "os.makedirs('dados_tratados', exist_ok=True)\n",
    "\n",
    "# Salvar em CSV\n",
    "caminho_csv = 'dados_tratados/dados_combustiveis_2011.csv'\n",
    "df_completo.to_csv(caminho_csv, index=False, encoding='utf-8', sep=';', decimal=',')\n",
    "\n",
    "# Salvar em Parquet\n",
    "caminho_parquet = 'dados_tratados/dados_combustiveis_2011.parquet'\n",
    "df_completo.to_parquet(caminho_parquet, index=False)\n",
    "\n",
    "# Mostrar informa√ß√µes\n",
    "print(f\"‚úÖ Dados salvos em CSV: {caminho_csv}\")\n",
    "print(f\"‚úÖ Dados salvos em Parquet: {caminho_parquet}\")\n",
    "\n",
    "# Verificar tamanho dos arquivos\n",
    "tamanho_csv = os.path.getsize(caminho_csv) / (1024 * 1024)\n",
    "tamanho_parquet = os.path.getsize(caminho_parquet) / (1024 * 1024)\n",
    "print(f\"\\nTamanho dos arquivos:\")\n",
    "print(f\"- CSV: {tamanho_csv:.2f} MB\")\n",
    "print(f\"- Parquet: {tamanho_parquet:.2f} MB\")\n",
    "print(f\"Economia: {((tamanho_csv - tamanho_parquet)/tamanho_csv*100):.1f}% de espa√ßo\")\n",
    "\n",
    "# Mostrar as primeiras linhas\n",
    "print(\"\\nPrimeiras linhas do dataframe:\")\n",
    "display(df_completo.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_maximo = df_completo['valor_venda'].max()\n",
    "valor_minimo = df_completo['valor_venda'].min()\n",
    "\n",
    "print(f\"Valor m√°ximo: {valor_maximo}\")\n",
    "print(f\"Valor m√≠nimo: {valor_minimo}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
